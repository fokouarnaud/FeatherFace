# FeatherFace Nano-B Architecture

## Overview

FeatherFace Nano-B is an ultra-lightweight face detection model that combines Bayesian-Optimized Soft FPGM Pruning with Weighted Knowledge Distillation to achieve 120-180K parameters (50-66% reduction from baseline) while maintaining competitive accuracy.

## Architecture Components

### 1. MobileNetV1 0.25× Backbone (~85K parameters)
- Lightweight feature extractor optimized for mobile deployment
- 0.25× width multiplier for ultra-efficiency
- Depthwise separable convolutions for parameter reduction

### 2. Efficient Modules Stack
- **Efficient CBAM** (~8K parameters): Channel and spatial attention with reduction ratio 8
- **Efficient BiFPN** (~45K parameters): Bi-directional feature pyramid with 72 channels
- **Grouped SSH** (~25K parameters): Grouped single-shot hierarchical detection with 2 groups

### 3. Detection Heads (~25K parameters)
- Bounding box regression
- Classification (face/background)
- Facial landmark detection (5 points)

## Scientific Innovations

### B-FPGM Bayesian Pruning
Based on Kaparinos & Mezaris (WACVW 2025):
- **Adaptive pruning rates**: 15-25% (backbone), 20-30% (CBAM), 25-35% (BiFPN), 10-20% (SSH), 5-15% (heads)
- **Bayesian optimization**: Automated rate selection for optimal accuracy-efficiency trade-off
- **Soft pruning**: Gradual weight reduction during training

### Weighted Knowledge Distillation
Based on Li et al. (CVPR 2023) and 2025 edge computing research:
- **Teacher model**: FeatherFace V1 (487K parameters)
- **Temperature**: 4.0 for optimal knowledge transfer
- **Alpha**: 0.7 (70% distillation, 30% task loss)
- **Adaptive weights**: Learnable coefficients for different output types

## Training Pipeline

### Phase 1: Weighted Knowledge Distillation (Epochs 1-100)
- Transfer knowledge from V1 teacher to Nano-B student
- Establish base capabilities before pruning

### Phase 2: Bayesian Pruning Optimization (Epochs 101-200)
- Apply B-FPGM with Bayesian-optimized rates
- Target 50% parameter reduction with minimal accuracy loss

### Phase 3: Fine-tuning (Epochs 201-300)
- Stabilize pruned weights
- Recover accuracy after structural changes

## Performance Targets

- **Parameters**: 120-180K (50-66% reduction from V1)
- **Model size**: <1 MB (ultra-lightweight)
- **WIDERFace mAP**: >78% overall (competitive with larger models)
- **Inference speed**: <50ms on mobile devices

## Deployment

The model supports:
- **Dynamic ONNX export**: Flexible input sizes (320×320 to 832×832)
- **Mobile optimization**: TorchScript for iOS/Android
- **Cross-platform**: Web deployment via ONNX.js
- **Edge devices**: Optimized for low-power inference

## Files Generated

- `featherface_nano_b_architecture.png`: High-resolution diagram
- `featherface_nano_b_architecture.pdf`: Vector format for publications
- `featherface_nano_b_architecture.svg`: Scalable web format

---

*Generated by FeatherFace Nano-B Architecture Generator*
