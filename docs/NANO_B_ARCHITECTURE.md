# FeatherFace Nano-B Architecture

## Overview

FeatherFace Nano-B uses an Enhanced-First strategy: **start with Enhanced architecture** (all 2024 modules active: ScaleDecoupling + ASSN + MSE-FPN + V1 base), then apply Bayesian-Optimized Soft FPGM Pruning to achieve 120-180K parameters (80-81% intelligent reduction) while preserving the best components through automated optimization.

## Architecture Strategy

### Phase 1: Enhanced Architecture Start (~619K parameters)
- **V1 Base Foundation**: MobileNetV1 0.25×, CBAM, BiFPN, SSH, Detection Heads (out_channel=56)
- **ScaleDecoupling Module**: P3 small/large object separation (SNLA 2024)
- **ASSN Module**: P3 specialized attention (PMC/ScienceDirect 2024)  
- **MSE-FPN Module**: Multi-scale semantic enhancement (Scientific Reports 2024)
- **ChannelShuffle**: Parameter-free information mixing (preserved from V1)
- **Complete Integration**: All 2024 modules active on proven V1 foundation

### Phase 2: Bayesian Pruning Analysis
- **B-FPGM Algorithm**: Analyzes complete Enhanced architecture importance automatically
- **Intelligent Decisions**: Which 2024 modules to keep? Which V1 components to preserve? Which channels to reduce?
- **Automated Optimization**: No manual cuts, AI-driven reduction of all 619K components
- **Preserved Best**: Keeps most valuable components from Enhanced architecture

### Phase 3: Final Pruned Enhanced Architecture (120-180K parameters)
- **Intelligent Enhanced Pruning**: Preserves most valuable Enhanced components
- **Smart Module Selection**: Automated decision on which 2024 modules to keep/reduce
- **Optimized V1 Base**: Efficient core while preserving proven architecture
- **Preserved Best Techniques**: Keeps most impactful optimization techniques from Enhanced

## Scientific Innovations

### B-FPGM Bayesian Pruning
Based on Kaparinos & Mezaris (WACVW 2025):
- **Adaptive pruning rates**: 15-25% (backbone), 20-30% (CBAM), 25-35% (BiFPN), 10-20% (SSH), 5-15% (heads)
- **Bayesian optimization**: Automated rate selection for optimal accuracy-efficiency trade-off
- **Soft pruning**: Gradual weight reduction during training

### Weighted Knowledge Distillation
Based on Li et al. (CVPR 2023) and 2025 edge computing research:
- **Teacher model**: FeatherFace V1 (487K parameters)
- **Temperature**: 4.0 for optimal knowledge transfer
- **Alpha**: 0.7 (70% distillation, 30% task loss)
- **Adaptive weights**: Learnable coefficients for different output types

## Training Pipeline

### Phase 1: Weighted Knowledge Distillation (Epochs 1-100)
- Transfer knowledge from V1 teacher to Nano-B student
- Establish base capabilities before pruning

### Phase 2: Bayesian Pruning Optimization (Epochs 101-200)
- Apply B-FPGM with Bayesian-optimized rates
- Target 50% parameter reduction with minimal accuracy loss

### Phase 3: Fine-tuning (Epochs 201-300)
- Stabilize pruned weights
- Recover accuracy after structural changes

## Performance Targets

- **Parameters**: 120-180K (50-66% reduction from V1)
- **Model size**: <1 MB (ultra-lightweight)
- **WIDERFace mAP**: >78% overall (competitive with larger models)
- **Inference speed**: <50ms on mobile devices

## Deployment

The model supports:
- **Dynamic ONNX export**: Flexible input sizes (320×320 to 832×832)
- **Mobile optimization**: TorchScript for iOS/Android
- **Cross-platform**: Web deployment via ONNX.js
- **Edge devices**: Optimized for low-power inference

## Files Generated

- `featherface_nano_b_architecture.png`: High-resolution diagram
- `featherface_nano_b_architecture.pdf`: Vector format for publications
- `featherface_nano_b_architecture.svg`: Scalable web format

---

*Generated by FeatherFace Nano-B Architecture Generator*
