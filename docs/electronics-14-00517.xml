<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">electronics</journal-id>
      <journal-title-group>
        <journal-title>Electronics</journal-title>
        <abbrev-journal-title abbrev-type="publisher">Electronics</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="pubmed">Electronics</abbrev-journal-title>
      </journal-title-group>
      <issn pub-type="epub">2079-9292</issn>
      <publisher>
        <publisher-name>MDPI</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="doi">10.3390/electronics14030517</article-id>
      <article-id pub-id-type="publisher-id">electronics-14-00517</article-id>
      <article-categories>
        <subj-group>
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>FeatherFace: Robust and Lightweight Face Detection via Optimal Feature Integration</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>Dohun</given-names>
          </name>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x2013; review &amp; editing</role>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4962-8347</contrib-id>
          <name>
            <surname>Jung</surname>
            <given-names>Jinmyung</given-names>
          </name>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x2013; review &amp; editing</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
          <xref rid="c1-electronics-14-00517" ref-type="corresp">*</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1835-7142</contrib-id>
          <name>
            <surname>Kim</surname>
            <given-names>Jinhyun</given-names>
          </name>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x2013; original draft</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x2013; review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x2013; review &amp; editing</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
          <xref rid="c1-electronics-14-00517" ref-type="corresp">*</xref>
        </contrib>
      </contrib-group>
      <contrib-group>
        <contrib contrib-type="editor">
          <name>
            <surname>Ullo</surname>
            <given-names>Silvia Liberata</given-names>
          </name>
          <role>Academic Editor</role>
        </contrib>
      </contrib-group>
      <aff id="af1-electronics-14-00517">Division of Data Science, College of Intelligent Software Convergence, The University of Suwon, Hwaseong 18323, Republic of Korea; <email>dohun0714@naver.com</email></aff>
      <author-notes>
        <corresp id="c1-electronics-14-00517"><label>*</label>Correspondence: <email>jmjung@suwon.ac.kr</email> (J.J.); <email>jinhyun.kim@suwon.ac.kr</email> (J.K.)</corresp>
      </author-notes>
      <pub-date pub-type="epub">
        <day>27</day>
        <month>01</month>
        <year>2025</year>
      </pub-date>
      <pub-date pub-type="collection">
        <month>02</month>
        <year>2025</year>
      </pub-date>
      <volume>14</volume>
      <issue>3</issue>
      <elocation-id>517</elocation-id>
      <history>
        <date date-type="received">
          <day>10</day>
          <month>01</month>
          <year>2025</year>
        </date>
        <date date-type="rev-recd">
          <day>24</day>
          <month>01</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>26</day>
          <month>01</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>&#xA9; 2025 by the authors.</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access">
          <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Face detection in resource-constrained environments presents challenges, due to the computational demands of state-of-the-art models and the complexity of real-world conditions, such as variations in scale, pose, and occlusion. This study introduces FeatherFace, a lightweight face-detection architecture with only 0.49 M parameters, designed for high accuracy and efficiency in such environments. Leveraging MobileNet-0.25 as a backbone, FeatherFace incorporates advanced feature-integration strategies, including a bidirectional feature pyramid network (BiFPN), a convolutional block attention module (CBAM), deformable convolutions, and channel shuffling. Evaluated on the WIDERFace dataset, FeatherFace achieves an overall average precision (AP) of 87.2%, with notable performance gains of 4.0% AP on the Hard subset compared with the baseline. Ablation studies highlight the critical role of multiscale feature integration and the strategic placement of attention mechanisms in addressing detection challenges such as small or occluded faces. With its compact design and reduced inference time, FeatherFace bridges the gap between the reliability of computationally intensive models and the need for deploying robust models in highly resource-constrained environments, such as edge devices and embedded systems. This work provides valuable insights for developing robust and lightweight models suited to challenging real-world applications.</p>
      </abstract>
      <kwd-group>
        <kwd>face detection</kwd>
        <kwd>feature integration</kwd>
        <kwd>lightweight deep learning model</kwd>
      </kwd-group>
      <funding-group>
        <award-group>
          <funding-source>National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT)</funding-source>
          <award-id>NRF-2022R1C1C1008823</award-id>
        </award-group>
        <funding-statement>This work was supported by a National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (NRF-2022R1C1C1008823).</funding-statement>
      </funding-group>
    </article-meta>
  </front>
  <body>
    <sec id="sec1-electronics-14-00517" sec-type="intro">
      <title>1. Introduction</title>
      <p>Face detection serves as the cornerstone in numerous computer vision applications, including surveillance, authentication systems, expression analysis, and human&#x2013;computer interactions [<xref ref-type="bibr" rid="B1-electronics-14-00517">1</xref>,<xref ref-type="bibr" rid="B2-electronics-14-00517">2</xref>,<xref ref-type="bibr" rid="B3-electronics-14-00517">3</xref>,<xref ref-type="bibr" rid="B4-electronics-14-00517">4</xref>,<xref ref-type="bibr" rid="B5-electronics-14-00517">5</xref>,<xref ref-type="bibr" rid="B6-electronics-14-00517">6</xref>,<xref ref-type="bibr" rid="B7-electronics-14-00517">7</xref>,<xref ref-type="bibr" rid="B8-electronics-14-00517">8</xref>]. Recent advances in deep learning have significantly improved face detection accuracy; however, reliably detecting faces in unconstrained environments remains a formidable challenge [<xref ref-type="bibr" rid="B9-electronics-14-00517">9</xref>,<xref ref-type="bibr" rid="B10-electronics-14-00517">10</xref>,<xref ref-type="bibr" rid="B11-electronics-14-00517">11</xref>,<xref ref-type="bibr" rid="B12-electronics-14-00517">12</xref>,<xref ref-type="bibr" rid="B13-electronics-14-00517">13</xref>,<xref ref-type="bibr" rid="B14-electronics-14-00517">14</xref>,<xref ref-type="bibr" rid="B15-electronics-14-00517">15</xref>]. This complexity stems from extreme variations in factors such as scale, pose, occlusion, expression, and illumination associated with face detection. For example, faces in images are, on average, approximately five times smaller than general objects in standard training datasets, making the task inherently more challenging than generic object detection [<xref ref-type="bibr" rid="B11-electronics-14-00517">11</xref>,<xref ref-type="bibr" rid="B16-electronics-14-00517">16</xref>]. The WIDERFace dataset exemplifies these challenges, serving as a comprehensive benchmark for assessing face detection performance under such diverse and demanding conditions [<xref ref-type="bibr" rid="B17-electronics-14-00517">17</xref>].</p>
      <p>RetinaFace, one of the most well-known deep learning-based face detection methods, has demonstrated state-of-the-art performance on this challenging dataset [<xref ref-type="bibr" rid="B9-electronics-14-00517">9</xref>]. However, such high-performance models typically rely on computationally intensive backbones, such as ResNet50 or ResNet152, which demand substantial processing power and memory [<xref ref-type="bibr" rid="B18-electronics-14-00517">18</xref>,<xref ref-type="bibr" rid="B19-electronics-14-00517">19</xref>]. These requirements render them impractical for deployment in highly resource-constrained environments, such as edge devices and Internet of Things (IoT) platforms, where power, memory, and computational capacity are severely limited. Consequently, there is a growing demand for robust and efficient face detection models designed to operate accurately within these constraints, driven by the increasing adoption of edge-based applications, such as smart home systems, wearable devices, and the IoT [<xref ref-type="bibr" rid="B20-electronics-14-00517">20</xref>,<xref ref-type="bibr" rid="B21-electronics-14-00517">21</xref>,<xref ref-type="bibr" rid="B22-electronics-14-00517">22</xref>,<xref ref-type="bibr" rid="B23-electronics-14-00517">23</xref>].</p>
      <p>In response, recent research has focused on lightweight neural architectures capable of maintaining high detection accuracy with reduced computational demands. Notable approaches include employing compact backbone networks in conjunction with advanced multiscale feature aggregation modules, such as feature pyramid network (FPN), weighted-fusion feature pyramid network (WFPN), and bidirectional feature pyramid network (BiFPN), which have yielded substantial performance gains [<xref ref-type="bibr" rid="B24-electronics-14-00517">24</xref>,<xref ref-type="bibr" rid="B25-electronics-14-00517">25</xref>,<xref ref-type="bibr" rid="B26-electronics-14-00517">26</xref>]. Attention mechanisms, including the convolutional block attention module (CBAM), squeeze-and-excitation network (SENet), and separated and enhancement attention module (SEAM) have also been incorporated, to help models concentrate on salient facial features across different scales [<xref ref-type="bibr" rid="B27-electronics-14-00517">27</xref>,<xref ref-type="bibr" rid="B28-electronics-14-00517">28</xref>,<xref ref-type="bibr" rid="B29-electronics-14-00517">29</xref>]. Although many of these lightweight models reach parameter sizes ranging from approximately 1 M to 7 M [<xref ref-type="bibr" rid="B29-electronics-14-00517">29</xref>,<xref ref-type="bibr" rid="B30-electronics-14-00517">30</xref>,<xref ref-type="bibr" rid="B31-electronics-14-00517">31</xref>,<xref ref-type="bibr" rid="B32-electronics-14-00517">32</xref>,<xref ref-type="bibr" rid="B33-electronics-14-00517">33</xref>,<xref ref-type="bibr" rid="B34-electronics-14-00517">34</xref>,<xref ref-type="bibr" rid="B35-electronics-14-00517">35</xref>], they may still be excessively large for highly resource-constrained systems. Meanwhile, more recent approaches have demonstrated models with parameter counts of approximately 0.5 M or less, yet they often suffer from reduced performance, achieving only 82% to 84% overall AP on the WIDERFace dataset when evaluated with VGA-resolution input images [<xref ref-type="bibr" rid="B36-electronics-14-00517">36</xref>,<xref ref-type="bibr" rid="B37-electronics-14-00517">37</xref>,<xref ref-type="bibr" rid="B38-electronics-14-00517">38</xref>].</p>
      <p>Here, we introduce FeatherFace, an ultra-lightweight face detection architecture containing only 0.49 M parameters&#x2014;drastically smaller than RetinaFace&#x2019;s 25 M-parameter architecture. Despite its compact design and stringent computational constraints, FeatherFace achieves an improved overall AP of 87.2% in the WIDERFace dataset through optimal feature-integration strategies.</p>
      <p>Building upon the RetinaFace baseline, we replace the conventional computationally heavy ResNet backbone with MobileNet-0.25, achieving a 50-fold reduction in the parameter count. To maintain detection accuracy under the tighter parameter budget, we incorporate enhanced feature-integration strategies that effectively combine multiscale feature information and emphasize critical facial cues. This strategy yields a 4.0% improvement in AP on the WIDERFace Hard subset, underscoring the effectiveness of our lightweight yet robust design. Specifically, systematic evaluations of various multiscale feature aggregation methods and attention mechanisms reveal that incorporating the BiFPN and CBAM together results in a 2.3% AP gain on the WIDERFace Hard subset. Furthermore, the adoption of the channel shuffle module and deformable convolution in the detection heads provides an additional 1.7% increase in AP.</p>
      <p>By achieving a high detection accuracy with minimal computational overhead, our work facilitates the broader deployment of face detection systems, particularly in highly resource-constrained environments, such as edge devices and IoT platforms. We attribute these performance gains to the more effective integration of multiscale features and adaptive receptive fields, which are crucial for accurately detecting small faces, handling occlusions, and dealing with unfavorable illumination. Our ablation studies further validate this conclusion. We believe that the optimized feature-integration methodologies presented here can extend beyond face detection to other computer vision tasks, such as object detection and segmentation, particularly in similarly constrained computational settings.</p>
    </sec>
    <sec id="sec2-electronics-14-00517">
      <title>2. Materials and Methods</title>
      <sec id="sec2dot1-electronics-14-00517">
        <title>2.1. WIDERFace Dataset</title>
        <p>The WIDERFace dataset served as the primary training and evaluation benchmark for FeatherFace [<xref ref-type="bibr" rid="B17-electronics-14-00517">17</xref>]. Comprising 32,203 images and 393,703 annotated face bounding boxes, WIDERFace captures an extensive array of real-world complexities, such as scale, pose, occlusion, expression, illumination, and makeup, across 61 event categories (<xref ref-type="app" rid="app1-electronics-14-00517">Figure S1</xref>). The dataset was partitioned into training (40%), validation (10%), and test (50%) splits. During training, the model leveraged annotated bounding boxes, along with five key facial landmarks (eyes, nose, and mouth corners). WIDERFace also provides three difficulty subsets&#x2014;Easy, Medium, and Hard&#x2014;that reflect increasing levels of detection challenges. These subsets facilitated a thorough performance assessment of the trained model under diverse real-world conditions.</p>
      </sec>
      <sec id="sec2dot2-electronics-14-00517">
        <title>2.2. Model Architecture</title>
        <p>FeatherFace incorporates four interconnected components: a lightweight backbone for initial feature extraction, multiscale feature aggregation for integrating information across different resolutions, attention mechanisms that focus the model on pertinent spatial and channel-wise features, and a detection head for final bounding-box regression and classification (<xref ref-type="fig" rid="electronics-14-00517-f001">Figure 1</xref>a). This design achieves robust face detection while maintaining minimal computational overhead, making it well suited for resource-constrained environments.</p>
        <sec id="sec2dot2dot1-electronics-14-00517">
          <title>2.2.1. Backbone</title>
          <p>In this study, the MobileNet-0.25 backbone was selected for its exceptionally lightweight architecture and computational efficiency. By leveraging depthwise separable convolutions, this network substantially reduces the number of parameters and associated computational cost compared with traditional convolutional networks, while preserving robust feature-extraction capabilities. It has a parameter footprint that is approximately fifty times smaller than that of ResNet50, but maintains consistent performance in feature extraction. This compact design makes it particularly advantageous for deployment on mobile or embedded devices, where computational and memory resources are often constrained.</p>
        </sec>
        <sec id="sec2dot2dot2-electronics-14-00517">
          <title>2.2.2. Multiscale Feature Aggregation</title>
          <p>The MobileNet-0.25 backbone generates three levels of image feature pyramids: P3, P4, and P5. P5 represents the most semantically rich and abstracted features, but has the lowest spatial resolution. Conversely, P3 offers the highest spatial resolution with detailed fine-grained features, but lacks substantial semantic context. Therefore, effective aggregation of these feature levels is critical for robust face detection, particularly in addressing challenges posed by images with varying face sizes across images.</p>
          <p>FeatherFace employs the BiFPN for multiscale feature aggregation [<xref ref-type="bibr" rid="B25-electronics-14-00517">25</xref>]. The BiFPN enables efficient bidirectional information flow between higher- and lower-level feature maps through top-down and bottom-up pathways (<xref ref-type="fig" rid="electronics-14-00517-f001">Figure 1</xref>a). The top-down pathway enhances lower-level features by infusing them with semantic information from higher-level feature maps, while the bottom-up pathway transfers finer-grained details from lower-level feature maps to higher-level representations. The BiFPN further refines this process, using weighted feature fusion, where learnable weights dynamically adjust the relative importance of feature maps at different scales. This approach increases the adaptability of the fusion mechanism, enabling the network to capture intricate interdependencies across varying feature resolutions. Consequently, the combined feature maps exhibit improved richness and robustness, particularly in challenging multiscale detection scenarios.</p>
          <p>Skip connections complement the BiFPN by mitigating information loss during feature fusion. In the bottom-up pathway, they help preserve fine-grained details that would otherwise be lost in the aggregation process. Similarly, in the top-down pathway, skip connections help restore essential semantic information to lower-level features. Additionally, these connections address gradient vanishing issues in deep networks, improving training stability and convergence. When integrated with the BiFPN&#x2019;s bidirectional fusion, skip connections enable seamless and efficient information transfer across feature levels.</p>
          <p>Formally, given multi-scale feature maps <inline-formula><mml:math id="mm1"><mml:semantics><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mtext>&#xA0;</mml:mtext><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, where <inline-formula><mml:math id="mm2"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> represents the input feature map at level <inline-formula><mml:math id="mm3"><mml:semantics><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, the aggregated new feature map <inline-formula><mml:math id="mm4"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> is computed as follows:<disp-formula id="FD1-electronics-14-00517"><label>(1)</label><mml:math id="mm5" display="block"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#xB7;</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#xB7;</mml:mo><mml:mi>U</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x3F5;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="FD2-electronics-14-00517"><label>(2)</label><mml:math id="mm6" display="block"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x2032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#xB7;</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x2032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#xB7;</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x2032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#xB7;</mml:mo><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x2212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x2032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x2032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x2032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x3F5;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula>
          
          where <inline-formula><mml:math id="mm7"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> is the intermediate top-down feature at level <inline-formula><mml:math id="mm8"><mml:semantics><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, <inline-formula><mml:math id="mm9"><mml:semantics><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> denotes the convolutional operation, and <inline-formula><mml:math id="mm10"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="mm11"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x2032;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> are learnable weights. Max pooling is used for the <inline-formula><mml:math id="mm12"><mml:semantics><mml:mrow><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> operation in the bottom-down pathway, while <inline-formula><mml:math id="mm13"><mml:semantics><mml:mrow><mml:mi>&#x3F5;</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> ensures numerical stability.</p>
          <p>Depthwise separable convolutions are utilized for the convolutional operation to enhance computational efficiency, with batch normalization and SiLU (swish) activation applied after each convolution. This configuration delivers high performance while minimizing computational overhead, enabling the model to merge features effectively within a compact computational footprint. As a result, the architecture is highly suitable for deployment in resource-constrained environments.</p>
          <p>While the BiFPN was ultimately chosen for its superior performance, alternative multiscale feature aggregation strategies, such as the FPN and the WFPN, were also evaluated during development [<xref ref-type="bibr" rid="B24-electronics-14-00517">24</xref>,<xref ref-type="bibr" rid="B26-electronics-14-00517">26</xref>]. Through this comprehensive comparative analysis, this study aimed to identify the most optimized feature-integration strategies for this task.</p>
        </sec>
        <sec id="sec2dot2dot3-electronics-14-00517">
          <title>2.2.3. Attention Mechanism</title>
          <p>FeatherFace incorporates the CBAM to enhance feature representation and emphasize critical aspects for face detection [<xref ref-type="bibr" rid="B27-electronics-14-00517">27</xref>]. The CBAM dynamically refines feature maps by prioritizing significant information and suppressing irrelevant details, resulting in more robust feature representations and improved detection accuracy in challenging real-world scenarios.</p>
          <p>The CBAM introduces a dual attention mechanism that sequentially applies channel and spatial attention (<xref ref-type="fig" rid="electronics-14-00517-f001">Figure 1</xref>b). The channel attention module identifies the importance of individual feature map channels by employing global average pooling and max pooling. These operations aggregate spatial information across the feature map, producing scalar values that capture the significance of each channel. These values are processed through a multilayer perceptron (MLP) and normalized using a sigmoid activation function to generate channel attention weights. These weights are then applied to the input feature map via elementwise multiplication, selectively enhancing critical channels.</p>
          <p>The spatial attention module focuses on identifying and emphasizing salient spatial regions within the feature map. During the channel pool step, max pooling and average pooling are applied along the channel dimension to summarize critical information for each spatial location, producing two spatial summary maps. These maps are concatenated and passed through a convolutional layer to generate a spatial attention map, which highlights essential spatial regions. The spatial attention map is normalized to the range [0, 1] using a sigmoid activation function and applied elementwise to the input feature map, ensuring that significant spatial locations are emphasized while irrelevant areas are suppressed.</p>
          <p>Formally, given an input feature map <inline-formula><mml:math id="mm14"><mml:semantics><mml:mrow><mml:mi mathvariant="bold">F</mml:mi><mml:mo>&#x2208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#xD7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#xD7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, where <inline-formula><mml:math id="mm15"><mml:semantics><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, <inline-formula><mml:math id="mm16"><mml:semantics><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>, and <inline-formula><mml:math id="mm17"><mml:semantics><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> represent the number of channels, height, and width, respectively, CBAM computes a channel attention map <inline-formula><mml:math id="mm18"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x2208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#xD7;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#xD7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> and a spatial attention map <inline-formula><mml:math id="mm19"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:msub><mml:mo>&#x2208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#xD7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#xD7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula>, as follows:<disp-formula id="FD3-electronics-14-00517"><label>(3)</label><mml:math id="mm20" display="block"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#x3C3;</mml:mi><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="FD4-electronics-14-00517"><label>(4)</label><mml:math id="mm21" display="block"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">F</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#x3C3;</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#xD7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:mfenced><mml:mo>;</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula>
          
          where <inline-formula><mml:math id="mm22"><mml:semantics><mml:mrow><mml:mi>&#x3C3;</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> denotes the sigmoid function, <inline-formula><mml:math id="mm23"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#xD7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> represents a convolution operation with a filter size of <inline-formula><mml:math id="mm24"><mml:semantics><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#xD7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>, and <inline-formula><mml:math id="mm25"><mml:semantics><mml:mrow><mml:mo>[</mml:mo><mml:mo>&#xB7;</mml:mo><mml:mtext>&#xA0;</mml:mtext><mml:mo>;</mml:mo><mml:mtext>&#xA0;</mml:mtext><mml:mo>&#xB7;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> denotes concatenation. <inline-formula><mml:math id="mm26"><mml:semantics><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="mm27"><mml:semantics><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> operations for computing the spatial attention map are applied along the channel dimension, while these operations for the channel attention map involve pooling across the spatial dimensions.</p>
          <p>The output <inline-formula><mml:math id="mm28"><mml:semantics><mml:mrow><mml:mi mathvariant="bold">F</mml:mi><mml:mo>&#x2033;</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> of the CBAM is computed as<disp-formula id="FD5-electronics-14-00517"><label>(5)</label><mml:math id="mm29" display="block"><mml:semantics><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x2032;</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">F</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x2299;</mml:mo><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:semantics></mml:math></disp-formula><disp-formula id="FD6-electronics-14-00517"><label>(6)</label><mml:math id="mm30" display="block"><mml:semantics><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x2033;</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">F</mml:mi><mml:mo>&#x2032;</mml:mo><mml:mo>)</mml:mo><mml:mo>&#x2299;</mml:mo><mml:mi mathvariant="bold">F</mml:mi><mml:mo>&#x2032;</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula>
          
          
          where <inline-formula><mml:math id="mm31"><mml:semantics><mml:mrow><mml:mo>&#x2299;</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> denotes element-wise multiplication.</p>
          <p>While the CBAM was selected for FeatherFace due to its superior performance, alternative attention mechanisms, such as the SENet, were also evaluated during development, to identify the most optimal approach [<xref ref-type="bibr" rid="B28-electronics-14-00517">28</xref>]. By incorporating CBAM, FeatherFace achieves an excellent balance between enhanced feature representation and computational efficiency, further reinforcing its suitability for resource-constrained environments.</p>
        </sec>
        <sec id="sec2dot2dot4-electronics-14-00517">
          <title>2.2.4. Detection Head</title>
          <p>The detection head is a critical component responsible for the final regression of bounding boxes and the classification of detected faces. It integrates a context enhancement module and a channel shuffling module to enrich feature representations, enabling the precise localization and accurate detection of faces. These modules work in tandem to capture diverse contextual information and promote effective inter-channel interactions, ensuring robust performance while maintaining computational efficiency.</p>
          <p>The context enhancement module plays a pivotal role in extracting multiscale contextual features essential for accurately delineating bounding boxes. Inspired by the single-stage headless (SSH) framework [<xref ref-type="bibr" rid="B10-electronics-14-00517">10</xref>], this module employs multiple convolutions with varying receptive fields&#x2014;such as 3 &#xD7; 3, 5 &#xD7; 5, and 7 &#xD7; 7&#x2014;to process features at different spatial scales (<xref ref-type="fig" rid="electronics-14-00517-f001">Figure 1</xref>c). This multi-receptive field approach equips the network to capture diverse object contexts effectively, which is critical for accurately identifying faces across varying sizes and poses.</p>
          <p>Additionally, we explored incorporating deformable convolutional networks (DCNs) as a replacement for standard convolutions, to further enhance the flexibility and adaptability of convolution operations [<xref ref-type="bibr" rid="B39-electronics-14-00517">39</xref>,<xref ref-type="bibr" rid="B40-electronics-14-00517">40</xref>]. Standard convolutions rely on fixed kernel sizes and shapes (e.g., 3 &#xD7; 3 and 5 &#xD7; 5) to sample input data, which inherently limits their ability to effectively model non-rigid transformations and complex geometric patterns. In contrast, deformable convolution overcomes this limitation by introducing learnable offsets to each kernel position. These offsets allow the sampling locations to be dynamically adjusted based on the input data, enabling the network to capture broader and more adaptable receptive fields. This capability significantly improves the model&#x2019;s ability to represent intricate object structures and handle diverse geometric transformations, thereby enhancing its effectiveness in challenging detection scenarios.</p>
          <p>The channel shuffle module, inspired by ShuffleNet V2 [<xref ref-type="bibr" rid="B41-electronics-14-00517">41</xref>], was employed to enhance feature representation by efficiently facilitating information exchange between channels in a lightweight deep learning architecture. This technique aims to increase the network&#x2019;s representational power while minimizing computational overhead. The channel shuffle mechanism operates by dividing the input tensor into multiple groups and shuffling the channels across these groups to promote stronger inter-channel interactions (<xref ref-type="fig" rid="electronics-14-00517-f001">Figure 1</xref>c). This is followed by a 1 &#xD7; 1 convolution to reduce channel dimensions, a depthwise convolution (3 &#xD7; 3 kernel, stride 1) for spatial refinement, and another 1 &#xD7; 1 convolution to restore the channel count. This approach boosts representational power while maintaining computational efficiency, making it ideal for resource-constrained applications.</p>
        </sec>
      </sec>
      <sec id="sec2dot3-electronics-14-00517">
        <title>2.3. Training Protocol</title>
        <p>The model was trained using fixed input image sizes of 640 &#xD7; 640. Data augmentation techniques, including random cropping, color distortion (brightness, contrast, and saturation adjustments), expansion, horizontal flipping, padding to square, resizing, and normalization, were employed to increase the diversity of the training data. An initial learning rate of 0.001 was used, with dynamic adjustments governed by the OneCycleLR scheduler. Optimization was performed using the AdamW optimizer. Training was conducted on an NVIDIA RTX A6000 GPU with 48 GB of memory, utilizing a batch size of 32 for 350 epochs.</p>
        <sec>
          <title>Loss Function</title>
          <p>The training employed a multitask loss function comprising localization loss (<inline-formula><mml:math id="mm32"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>), classification loss (<inline-formula><mml:math id="mm33"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>), and landmark loss (<inline-formula><mml:math id="mm34"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>). The total loss <inline-formula><mml:math id="mm35"><mml:semantics><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> was defined as <inline-formula><mml:math id="mm36"><mml:semantics><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#xD7;</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, prioritizing localization accuracy.</p>
          <p>The localization loss aims to minimize positional discrepancies between the predicted bounding boxes and ground-truth bounding boxes. The intersection over union (IoU) metric was used to measure the overlap between prior boxes and ground-truth bounding boxes. Prior boxes with an IoU exceeding the threshold of 0.35 are designated as positive samples, whereas the other boxes are treated as negatives. For the positive boxes, Smooth L1 loss was applied to compute the difference between the predicted and ground-truth coordinates, effectively refining the bounding-box predictions. The relatively low IoU threshold of 0.35 was employed to learn from a wider range of examples, including those with less precise overlaps, thereby enhancing its robustness and generalization capability.</p>
          <p>The classification loss focused on reducing the discrepancy between the predicted class probabilities and ground-truth labels. Positive samples, identified based on the IoU and threshold criteria, were linked to their corresponding ground-truth labels. The hard negative mining technique was applied to address the imbalance caused by excessive negative samples, selecting a subset of negative samples with the highest loss values for inclusion in training. Both positive and selected negative samples were then processed using the Cross Entropy loss function to calculate the classification loss, ensuring the model&#x2019;s ability to distinguish facial features effectively.</p>
          <p>The landmark loss facilitated the precise prediction of facial landmarks, such as the eyes, nose, and lips, which are essential for face detection and recognition tasks. Training on landmark coordinates was restricted to positive samples that met the IoU threshold (IoU &#x2265; 0.35). Smooth L1 loss was employed to compute the differences between the predicted and ground-truth landmark coordinates. The loss was summed across all positive samples and normalized by their total count, to derive the final landmark loss. When optimized alongside localization and classification losses, landmark loss enhances the model&#x2019;s ability to accurately estimate facial features, improving overall detection performance.</p>
        </sec>
      </sec>
      <sec id="sec2dot4-electronics-14-00517">
        <title>2.4. Evaluation</title>
        <p>The trained FeatherFace model was evaluated using the original images without scaling, with the pixel values normalized by subtracting the mean values [104, 117, 123]. The evaluation procedure comprised three sequential steps: applying the non-maximum suppression (NMS) algorithm, generating the precision&#x2013;recall curve, and calculating the average precision (AP).</p>
        <p>NMS was utilized to eliminate redundant bounding boxes while retaining only the most confident detections. Specifically, bounding boxes with confidence scores below 0.02 were discarded. Among the remaining boxes, those with an intersection over union (IoU) value greater than 0.4 were suppressed, to ensure unique and accurate predictions.</p>
        <p>For the precision&#x2013;recall curve, precision and recall values were calculated at confidence thresholds ranging from 0.02 to 1.0, in increments of 0.001. Precision was defined as TP/(TP + FP) and recall as TP/(TP + FN), where TP, FP, and FN represent the counts of true positives, false positives, and false negatives, respectively. A detection was considered a true positive if its IoU with the ground truth was 0.5 or higher. By plotting recall on the <italic>x</italic>-axis and precision on the <italic>y</italic>-axis, the curve illustrates the trade-off between precision and recall across varying confidence levels.</p>
        <p>The AP was determined as the area under the precision&#x2013;recall curve across all evaluated images. A stricter IoU threshold of 0.5 was applied during the evaluation, to ensure that only detections with a high overlap with the ground truth were considered true positives. In comparison, the IoU threshold for NMS (0.4) was an intermediate value between the training threshold (0.35) and the AP calculation threshold (0.5), balancing the removal of redundant detections with the preservation of valid ones.</p>
        <p>Finally, the number of floating-point operations (FLOPs) and total parameters were measured for an input image of size 640 &#xD7; 640, to evaluate the computational efficiency and model compactness. This analysis provides practical insight into the trade-off between detection accuracy and computational resource demands, offering a comprehensive understanding of the model&#x2019;s suitability for resource-constrained applications.</p>
      </sec>
    </sec>
    <sec id="sec3-electronics-14-00517" sec-type="results">
      <title>3. Results</title>
      <p>To rigorously assess the effectiveness of the proposed FeatherFace architecture, comprehensive experiments were conducted on the WIDERFace benchmark dataset. <xref ref-type="table" rid="electronics-14-00517-t001">Table 1</xref> illustrates incremental modifications&#x2014;from a simplified RetinaFace baseline to the fully optimized FeatherFace&#x2014;and demonstrates the resulting improvements in detection accuracy and model compactness.</p>
      <sec id="sec3dot1-electronics-14-00517">
        <title>3.1. Performance Evaluation of FeatherFace Architecture</title>
        <p>The baseline configuration employed a MobileNet-0.25 backbone with FPN-based feature aggregation and utilized conventional convolutional heads (Conv) for classification and bounding-box regression. This simplified RetinaFace setup achieved AP values of 87.5% on the Medium subset and 74.3% on the Hard subset (<xref ref-type="table" rid="electronics-14-00517-t001">Table 1</xref>). While this served as a reasonable starting point, the baseline exhibited limitations when dealing with small or occluded faces, and lacked the computational efficiency demanded by resource-constrained environments.</p>
        <p>To enhance the model&#x2019;s ability to effectively combine multiscale facial features extracted from the backbone, the baseline FPN was replaced with the BiFPN, a feature aggregation module designed to adaptively learn the relative importance of features at different scales (<xref ref-type="fig" rid="electronics-14-00517-f002">Figure 2</xref>a). As shown in <xref ref-type="table" rid="electronics-14-00517-t001">Table 1</xref>, the BiFPN provided consistent performance gains across the Easy, Medium, and Hard subsets. For instance, the Medium subset AP improved from 87.5% to 88.3%, indicating a clear advantage over the baseline. In contrast, substituting the FPN with the WFPN showed negligible benefits, suggesting that not all multiscale feature-integration methods are equally effective in ultra-lightweight settings.</p>
        <p>To further refine the model&#x2019;s feature representation, attention mechanisms were incorporated at various stages of the network (<xref ref-type="fig" rid="electronics-14-00517-f002">Figure 2</xref>a). Integrating the CBAM into both the backbone and BiFPN outputs led to significant performance gains, particularly on the Hard subset, where AP improved from 74.6% to 76.6% (<xref ref-type="table" rid="electronics-14-00517-t001">Table 1</xref>). Placing the CBAM after both the backbone and BiFPN resulted in the most substantial improvements, outperforming other configurations such as positioning the module exclusively after the backbone or across multiple network stages. These results emphasize the importance of leveraging attention mechanisms to enhance feature representation by emphasizing informative spatial and channel-wise features.</p>
        <p>The final FeatherFace architecture further incorporates deformable convolutions and channel shuffling, leading to a substantial performance boost (<xref ref-type="fig" rid="electronics-14-00517-f002">Figure 2</xref>a). FeatherFace achieves AP scores of 92.7%, 90.7%, and 78.3% on the Easy, Medium, and Hard subsets, respectively, demonstrating a clear advantage over both the baseline and intermediate configurations. The total 4.0% AP improvement on the WIDERFace Hard subset over the simplified RetinaFace baseline underscores the robustness of the FeatherFace architecture in challenging face-detection scenarios. Beyond these AP enhancements, FeatherFace demonstrates superior precision&#x2013;recall curves compared with all intermediate and baseline architectures, further supporting its effectiveness and reliability (<xref ref-type="fig" rid="electronics-14-00517-f002">Figure 2</xref>b). These trends are consistently observed across individual analyses of the Easy, Medium, and Hard subsets, as highlighted in <xref ref-type="app" rid="app1-electronics-14-00517">Figure S2</xref>, reinforcing the model&#x2019;s comprehensive robustness across diverse detection challenges.</p>
        <p>In summary, FeatherFace achieves an overall AP of 87.2% on the WIDERFace dataset, while maintaining a parameter count of just 0.489 M&#x2014;a substantial reduction compared with the 25 M parameters of the original RetinaFace. This compact architecture makes the model exceptionally well suited for deployment in resource-constrained environments and applications requiring real-time performance. These results demonstrate that the optimized feature-integration strategies&#x2014;including multiscale feature aggregation, attention mechanisms, deformable convolutions, and channel shuffling&#x2014;are instrumental in achieving high detection accuracy while substantially minimizing computational resource requirements.</p>
      </sec>
      <sec id="sec3dot2-electronics-14-00517">
        <title>3.2. Robustness of FeatherFace Across Six Challenging Scenarios</title>
        <p>To evaluate the robustness of FeatherFace, we analyzed its performance across six challenging scenarios in the WIDERFace Hard subset: blur, expression, illumination, occlusion, pose, and scale. These scenarios represent common challenges in real-world face detection, where varying image qualities and conditions can significantly impact detection accuracy.</p>
        <p>As illustrated in <xref ref-type="fig" rid="electronics-14-00517-f003">Figure 3</xref>a, FeatherFace consistently outperformed the simplified RetinaFace baseline across all six scenarios. Notable improvements were observed in handling expression variations, where FeatherFace achieved a significantly higher AP of 95.5% compared with the baseline&#x2019;s 88.5%. Additionally, FeatherFace demonstrated superior performance in detecting occluded and smaller faces, with AP scores of 64.3% and 62.9%, respectively, compared with the baseline&#x2019;s 57.6% and 58.4%, respectively.</p>
        <p>In addition to the quantitative results, the qualitative comparisons depicted in <xref ref-type="fig" rid="electronics-14-00517-f003">Figure 3</xref>b illustrate FeatherFace&#x2019;s enhanced bounding-box precision relative to the baseline. FeatherFace consistently identified a greater number of correctly detected faces in challenging scenarios such as occluded, blurred, or smaller faces, effectively reducing both false positives and false negatives. These visual examples further support the quantitative findings and highlight the practical advantages of FeatherFace in real-world applications.</p>
        <p>These results validate FeatherFace&#x2019;s robustness across a diverse range of challenging scenarios, including exaggerated expressions, partially occluded faces, numerous small faces, and motion or focus blur. Its consistent outperformance of the baseline highlights the effectiveness of optimized feature-integration strategies in enhancing facial representation, thereby addressing real-world challenges in face detection.</p>
      </sec>
      <sec id="sec3dot3-electronics-14-00517">
        <title>3.3. Ablation Studies</title>
        <p>An ablation study was conducted to evaluate the influence of key design choices in the FeatherFace architecture, focusing on two critical aspects: multiscale feature inputs to the BiFPN and the depth of the BiFPN layers.</p>
        <sec id="sec3dot3dot1-electronics-14-00517">
          <title>3.3.1. Multiscale Feature Inputs to Bidirectional Feature Pyramid Network (BiFPN)</title>
          <p>The BiFPN module originally utilized multiscale feature inputs from three feature pyramid levels, denoted P3, P4, and P5, where P3 represents the feature level with the highest spatial resolution. To assess the contribution of each feature level, various configurations were tested by selectively excluding certain inputs. The tested configurations included single-level inputs (P3), (P4), and (P5), pairwise combinations (P3, P4), (P4, P5), and (P3, P5), and the original full input configuration (P3, P4, P5).</p>
          <p>The BiFPN layers were removed when a single feature level was used, as there was no need for multiscale aggregation. The results show that P3, the highest-resolution feature level, performed better on the Hard subset (AP of 60.4%) than P5, which achieved only 46.9% (<xref ref-type="table" rid="electronics-14-00517-t002">Table 2</xref>). This outcome aligns with our expectations, as the WIDERFace Hard subset predominantly includes smaller faces, where features with high spatial resolution (such as those from P3) are better suited for accurate detection. Conversely, P5, which represents the lowest resolution but more semantically rich feature level, delivered the highest performance on the Easy subset, achieving an AP of 40.6%, significantly outperforming P3, which achieved only 23.0%. This result suggests that lower-resolution features are more effective for detecting larger faces, which are more prevalent in the Easy subset.</p>
          <p>The pairwise combinations of the feature levels demonstrated similar trends. Combinations that included P3 consistently achieved higher performance on the Hard subset, whereas those that included P5 showed superior performance on the Easy subset. Notably, pairwise combinations of feature levels resulted in significantly higher AP scores (average of 80.7%) than configurations using single feature levels (average of 48.4%). This finding underscores the importance of optimally integrating features from multiple scales to address the diverse face sizes and scales present in real-world datasets.</p>
        </sec>
        <sec id="sec3dot3dot2-electronics-14-00517">
          <title>3.3.2. Reduction in Number of BiFPN Layers</title>
          <p>The original three-layer configuration was adjusted to include one, two, and four layers, to analyze the effect of the BiFPN layer depth. Overall, the number of BiFPN layers had a relatively minor impact on the detection performance, with AP differences between configurations being less than 0.4% in any pairwise comparison (<xref ref-type="table" rid="electronics-14-00517-t002">Table 2</xref>). The three-layer BiFPN configuration achieved the highest AP performance across subsets, but the single-layer BiFPN configuration offered a viable alternative for scenarios prioritizing model compactness, with only a marginal decrease in performance.</p>
        </sec>
      </sec>
      <sec id="sec3dot4-electronics-14-00517">
        <title>3.4. Comparative Analysis with Existing Lightweight Face Detectors</title>
        <p>To further evaluate the performance and efficiency of FeatherFace, we compared it with various lightweight and high-performance face detectors, as summarized in <xref ref-type="table" rid="electronics-14-00517-t003">Table 3</xref>. The comparison includes complex architectures with high parameter counts (e.g., RetinaFace), lightweight models with medium parameter counts ranging from 1 M to 7 M, and ultra-lightweight models with approximately 0.5 M parameters or fewer.</p>
        <p>FeatherFace demonstrated competitive performance among models with similar parameter counts and computational budgets. Specifically, it achieved an overall AP of 87.2% with just 0.49 M parameters, compared with SCRFD-0.5GF (82.4%), MTCNN (75.9%), and YOLOv5n0.5 (84.2%). While heavier models such as RetinaFace delivered higher absolute AP scores, their significantly larger parameter counts and computational requirements make them less suitable for deployment in highly resource-constrained environments.</p>
        <p>These results suggest that FeatherFace effectively balances efficiency and detection accuracy, making it a strong candidate for deployment in real-world applications requiring lightweight and scalable solutions. By maintaining a minimal parameter count and competitive detection accuracy, FeatherFace highlights the potential of optimized feature-integration strategies for efficient face detection in resource-limited scenarios.</p>
      </sec>
      <sec id="sec3dot5-electronics-14-00517">
        <title>3.5. Generalization of FeatherFace Core Across Diverse Backbones</title>
        <p>To evaluate the versatility and effectiveness of the FeatherFace Core components&#x2014;comprising BiFPN, CBAM, deformable convolutions, and channel shuffling&#x2014;when applied to various backbone networks, additional experiments were conducted using four distinct backbones: Darknet, EfficientNet, MnasNet-0.5, and ShuffleNet-V2-0.25. <xref ref-type="table" rid="electronics-14-00517-t004">Table 4</xref> summarizes the comparative results, highlighting the performance improvements achieved by integrating the FeatherFace Core with each backbone.</p>
        <p>The results demonstrate consistent performance improvements across all backbones upon integrating the FeatherFace Core. Its application to MnasNet-0.5 resulted in a substantial increase in overall AP, from 63.0% to 86.2%. This improvement was achieved with only a marginal increase in computational complexity, as reflected in the modest changes to parameter count and FLOPs. Similar trends were observed across all other backbones, underscoring the versatility and effectiveness of the FeatherFace Core modules. This generalizability highlights the potential of the FeatherFace Core as a modular solution for building robust and efficient face detection models tailored to diverse deployment scenarios, including resource-constrained environments.</p>
      </sec>
    </sec>
    <sec id="sec4-electronics-14-00517" sec-type="discussion">
      <title>4. Discussion and Conclusions</title>
      <p>In this study, we introduced FeatherFace, a lightweight yet robust face detection framework designed for resource-constrained environments. FeatherFace leverages a combination of advanced feature-integration strategies, including multiscale feature aggregation, attention mechanisms, deformable convolutions, and channel shuffling, to achieve high detection accuracy with an exceptionally low parameter count of 0.49 M. These design choices address key challenges in face detection&#x2014;such as blurring, small faces, and partial occlusions&#x2014;making FeatherFace well suited for complex real-world applications. Furthermore, its compact size paves the way for broader deployment on edge devices, embedded systems, and real-time monitoring platforms where computational resources are severely limited.</p>
      <p>Building upon the strengths of state-of-the-art frameworks such as RetinaFace and leveraging lightweight backbones such as MobileNet, FeatherFace incorporates feature-integration strategies such as BiFPN, CBAM, DCN, and channel shuffling to jointly enhance detection robustness and computational efficiency. This combination of modules, collectively referred to as the FeatherFace Core, has also demonstrated its effectiveness and adaptability across diverse backbone architectures, as detailed in <xref ref-type="sec" rid="sec3dot5-electronics-14-00517">Section 3.5</xref>.</p>
      <p>Our experiments on the WIDERFace dataset highlight its notable performance gains, especially on the Hard subset, which represents the most challenging real-world scenarios. By achieving an overall AP of 87.2% with an exceptionally compact model size, FeatherFace bridges the gap between high-performing but computationally expensive models and those optimized for efficiency.</p>
      <p>Ablation studies revealed the pivotal role of strategic feature fusion, underscoring how high-resolution features (P3) help detect small faces, whereas more semantically rich, lower-resolution features (P4, P5) enhance the detection of larger faces. These findings illustrate how thoughtful multiscale feature integration can significantly boost face detection performance, especially under challenging face detection scenarios.</p>
      <p>Future work could explore smaller and more efficient backbone networks to further reduce parameter counts while sustaining performance. Automated neural architecture search (NAS) offers another promising direction for tailoring the FeatherFace design to diverse hardware profiles systematically. Knowledge distillation or model pruning may also be employed to maintain high accuracy with lower computational overhead and faster inference times.</p>
      <p>FeatherFace exemplifies how architectural optimizations can simultaneously achieve robustness and efficiency in face detection. These design principles can potentially be extended to a broader range of vision tasks, including object detection and semantic segmentation, particularly in similarly constrained computational environments. We believe that the strategies demonstrated in this work provide a foundation for developing resource-efficient and high-performing models capable of meeting the practical demands of real-world scenarios, thereby advancing the frontier of lightweight computer vision systems.</p>
    </sec>
  </body>
  <back>
    <app-group>
      <app id="app1-electronics-14-00517">
        <title>Supplementary Materials</title>
        <p>The following supporting information can be downloaded at: <uri>https://www.mdpi.com/article/10.3390/electronics14030517/s1</uri>, Figure S1: Representative images from the WIDERFace dataset showcasing various challenging scenarios in face detection, including scale, pose, occlusion, expression, makeup, and illumination; Figure S2: The phase-wise AP and the precision-recall curves for the Easy, Medium, and Hard subsets of the WIDERFace dataset.</p>
        <supplementary-material xmlns:xlink="http://www.w3.org/1999/xlink" id="electronics-14-00517-s001" xlink:href="electronics-14-00517-s001.zip"/>
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, J.J. and D.K.; methodology, D.K. and J.K.; software and experiments, D.K.; validation and investigation, J.K. and D.K.; writing&#x2014;original draft preparation, J.K.; writing&#x2014;review and editing, D.K., J.J. and J.K.; visualization, J.J.; supervision, J.J. and J.K.; project administration, J.J. and J.K.; funding acquisition, J.J. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The WIDERFace dataset can be accessed and downloaded from its official website (<uri>http://shuoyang1213.me/WIDERFACE</uri> (accessed on 25 January 2025)). The PyTorch implementation of FeatherFace is publicly available on GitHub (<uri>https://github.com/dohun-mat/FeatherFace</uri> (accessed on 25 January 2025)).</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflicts of interest.</p>
    </notes>
    <glossary>
      <title>Abbreviations</title>
      <p>The following abbreviations are used in this manuscript:
      <array><tbody><tr><td align="left" valign="middle">AP</td><td align="left" valign="middle">Average precision</td></tr><tr><td align="left" valign="middle">FPN</td><td align="left" valign="middle">Feature pyramid network</td></tr><tr><td align="left" valign="middle">BiFPN</td><td align="left" valign="middle">Bidirectional feature pyramid network</td></tr><tr><td align="left" valign="middle">CBAM</td><td align="left" valign="middle">Convolutional block attention module</td></tr><tr><td align="left" valign="middle">SENet</td><td align="left" valign="middle">Squeeze-and-excitation network</td></tr><tr><td align="left" valign="middle">SSH</td><td align="left" valign="middle">Single-stage headless</td></tr><tr><td align="left" valign="middle">DCN</td><td align="left" valign="middle">Deformable convolutional network</td></tr><tr><td align="left" valign="middle">NMS</td><td align="left" valign="middle">Non-maximum suppression</td></tr><tr><td align="left" valign="middle">FLOPs</td><td align="left" valign="middle">Floating-point operations</td></tr></tbody></array></p>
    </glossary>
    <ref-list>
      <title>References</title>
      <ref id="B1-electronics-14-00517">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Viola</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Robust Real-Time Face Detection</article-title>
          <source>Int. J. Comput. Vis.</source>
          <year>2004</year>
          <volume>57</volume>
          <fpage>137</fpage>
          <lpage>154</lpage>
          <pub-id pub-id-type="doi">10.1023/B:VISI.0000013087.49260.fb</pub-id>
        </element-citation>
      </ref>
      <ref id="B2-electronics-14-00517">
        <label>2.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Schroff</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Kalenichenko</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Philbin</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>FaceNet: A Unified Embedding for Face Recognition and Clustering</article-title>
          <source>Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <conf-loc>Boston, MA, USA</conf-loc>
          <conf-date>7&#x2013;12 June 2015</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2015</year>
          <fpage>815</fpage>
          <lpage>823</lpage>
        </element-citation>
      </ref>
      <ref id="B3-electronics-14-00517">
        <label>3.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Mao</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Joint Pose and Expression Modeling for Facial Expression Recognition</article-title>
          <source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
          <conf-loc>Salt Lake City, UT, USA</conf-loc>
          <conf-date>18&#x2013;23 June 2018</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2018</year>
          <fpage>3359</fpage>
          <lpage>3368</lpage>
        </element-citation>
      </ref>
      <ref id="B4-electronics-14-00517">
        <label>4.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Xue</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Zafeiriou</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>ArcFace: Additive Angular Margin Loss for Deep Face Recognition</article-title>
          <source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
          <conf-loc>Long Beach, CA, USA</conf-loc>
          <conf-date>15&#x2013;20 June 2019</conf-date>
        </element-citation>
      </ref>
      <ref id="B5-electronics-14-00517">
        <label>5.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>CosFace: Large Margin Cosine Loss for Deep Face Recognition</article-title>
          <source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
          <conf-loc>Salt Lake City, UT, USA</conf-loc>
          <conf-date>18&#x2013;23 June 2018</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2018</year>
          <fpage>5265</fpage>
          <lpage>5274</lpage>
        </element-citation>
      </ref>
      <ref id="B6-electronics-14-00517">
        <label>6.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Raj</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>SphereFace: Deep Hypersphere Embedding for Face Recognition</article-title>
          <source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <conf-loc>Honolulu, HI, USA</conf-loc>
          <conf-date>21&#x2013;26 July 2017</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2017</year>
          <fpage>6738</fpage>
          <lpage>6746</lpage>
        </element-citation>
      </ref>
      <ref id="B7-electronics-14-00517">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kong</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rocha</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kwong</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Detect and Locate: Exposing Face Manipulation by Semantic- and Noise-Level Telltales</article-title>
          <source>IEEE Trans. Inform. Forensic Secur.</source>
          <year>2022</year>
          <volume>17</volume>
          <fpage>1741</fpage>
          <lpage>1756</lpage>
          <pub-id pub-id-type="doi">10.1109/TIFS.2022.3169921</pub-id>
        </element-citation>
      </ref>
      <ref id="B8-electronics-14-00517">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yakovleva</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Kovtunenko</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Liubchenko</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Honcharenko</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Kobylin</surname>
              <given-names>O.</given-names>
            </name>
          </person-group>
          <article-title>Face Detection for Video Surveillance-Based Security System</article-title>
          <source>COLINS</source>
          <year>2023</year>
          <fpage>69</fpage>
          <lpage>86</lpage>
        </element-citation>
      </ref>
      <ref id="B9-electronics-14-00517">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kotsia</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Zafeiriou</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>RetinaFace: Single-Stage Dense Face Localisation in the Wild</article-title>
          <source>arXiv</source>
          <year>2019</year>
          <pub-id pub-id-type="arxiv">1905.00641</pub-id>
        </element-citation>
      </ref>
      <ref id="B10-electronics-14-00517">
        <label>10.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Najibi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Samangouei</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Chellappa</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>L.S.</given-names>
            </name>
          </person-group>
          <article-title>SSH: Single Stage Headless Face Detector</article-title>
          <source>Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</source>
          <conf-loc>Venice, Italy</conf-loc>
          <conf-date>22&#x2013;29 October 2017</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <fpage>4885</fpage>
          <lpage>4894</lpage>
        </element-citation>
      </ref>
      <ref id="B11-electronics-14-00517">
        <label>11.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ramanan</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Finding Tiny Faces</article-title>
          <source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <conf-loc>Honolulu, HI, USA</conf-loc>
          <conf-date>21&#x2013;26 July 2017</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2017</year>
          <fpage>1522</fpage>
          <lpage>1530</lpage>
        </element-citation>
      </ref>
      <ref id="B12-electronics-14-00517">
        <label>12.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S.Z.</given-names>
            </name>
          </person-group>
          <article-title>S3FD: Single Shot Scale-Invariant Face Detector</article-title>
          <source>Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</source>
          <conf-loc>Venice, Italy</conf-loc>
          <conf-date>22&#x2013;29 October 2017</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <fpage>192</fpage>
          <lpage>201</lpage>
        </element-citation>
      </ref>
      <ref id="B13-electronics-14-00517">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chi</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Xing</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S.Z.</given-names>
            </name>
            <name>
              <surname>Zou</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>Selective Refinement Network for High Performance Face Detection</article-title>
          <source>Proc. AAAI Conf. Artif. Intell.</source>
          <year>2019</year>
          <volume>33</volume>
          <fpage>8231</fpage>
          <lpage>8238</lpage>
          <pub-id pub-id-type="doi">10.1609/aaai.v33i01.33018231</pub-id>
        </element-citation>
      </ref>
      <ref id="B14-electronics-14-00517">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Minaee</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Bowyer</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Going Deeper Into Face Detection: A Survey</article-title>
          <source>arXiv</source>
          <year>2021</year>
          <pub-id pub-id-type="arxiv">2103.14983</pub-id>
        </element-citation>
      </ref>
      <ref id="B15-electronics-14-00517">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mamieva</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Abdusalomov</surname>
              <given-names>A.B.</given-names>
            </name>
            <name>
              <surname>Mukhiddinov</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Whangbo</surname>
              <given-names>T.K.</given-names>
            </name>
          </person-group>
          <article-title>Improved Face Detection Method via Learning Small Faces on Hard Images Based on a Deep Learning Approach</article-title>
          <source>Sensors</source>
          <year>2023</year>
          <volume>23</volume>
          <elocation-id>502</elocation-id>
          <pub-id pub-id-type="doi">10.3390/s23010502</pub-id>
        </element-citation>
      </ref>
      <ref id="B16-electronics-14-00517">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Tai</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Xia</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>ASFD: Automatic and Scalable Face Detector</article-title>
          <source>arXiv</source>
          <year>2022</year>
          <pub-id pub-id-type="arxiv">2201.1078</pub-id>
        </element-citation>
      </ref>
      <ref id="B17-electronics-14-00517">
        <label>17.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Loy</surname>
              <given-names>C.C.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>WIDER FACE: A Face Detection Benchmark</article-title>
          <source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <conf-loc>Las Vegas, NV, USA</conf-loc>
          <conf-date>27&#x2013;30 June 2016</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2016</year>
          <fpage>5525</fpage>
          <lpage>5533</lpage>
        </element-citation>
      </ref>
      <ref id="B18-electronics-14-00517">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Xiong</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>TinaFace: Strong but Simple Baseline for Face Detection</article-title>
          <source>arXiv</source>
          <year>2021</year>
          <pub-id pub-id-type="arxiv">2011.13183</pub-id>
        </element-citation>
      </ref>
      <ref id="B19-electronics-14-00517">
        <label>19.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>MogFace: Towards a Deeper Appreciation on Face Detection</article-title>
          <source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <conf-loc>New Orleans, LA, USA</conf-loc>
          <conf-date>18&#x2013;24 June 2022</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2022</year>
          <fpage>4083</fpage>
          <lpage>4092</lpage>
        </element-citation>
      </ref>
      <ref id="B20-electronics-14-00517">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Meddeb</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Abdellaoui</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Houaidi</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Development of Surveillance Robot Based on Face Recognition Using Raspberry-PI and IOT</article-title>
          <source>Microprocess. Microsyst.</source>
          <year>2023</year>
          <volume>96</volume>
          <fpage>104728</fpage>
          <pub-id pub-id-type="doi">10.1016/j.micpro.2022.104728</pub-id>
        </element-citation>
      </ref>
      <ref id="B21-electronics-14-00517">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>George</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ecabert</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Shahreza</surname>
              <given-names>H.O.</given-names>
            </name>
            <name>
              <surname>Kotwal</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Marcel</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>EdgeFace: Efficient Face Recognition Model for Edge Devices</article-title>
          <source>IEEE Trans. Biom. Behav. Identity Sci.</source>
          <year>2024</year>
          <volume>6</volume>
          <fpage>158</fpage>
          <lpage>168</lpage>
          <pub-id pub-id-type="doi">10.1109/TBIOM.2024.3352164</pub-id>
        </element-citation>
      </ref>
      <ref id="B22-electronics-14-00517">
        <label>22.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mohammad</surname>
              <given-names>A.S.</given-names>
            </name>
            <name>
              <surname>Jarullah</surname>
              <given-names>T.G.</given-names>
            </name>
            <name>
              <surname>Al-Kaltakchi</surname>
              <given-names>M.T.S.</given-names>
            </name>
            <name>
              <surname>Alshehabi Al-Ani</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Dey</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>IoT-MFaceNet: Internet-of-Things-Based Face Recognition Using MobileNetV2 and FaceNet Deep-Learning Implementations on a Raspberry Pi-400</article-title>
          <source>J. Low Power Electron. Appl.</source>
          <year>2024</year>
          <volume>14</volume>
          <elocation-id>46</elocation-id>
          <pub-id pub-id-type="doi">10.3390/jlpea14030046</pub-id>
        </element-citation>
      </ref>
      <ref id="B23-electronics-14-00517">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Naseri</surname>
              <given-names>R.A.S.</given-names>
            </name>
            <name>
              <surname>Kurnaz</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Farhan</surname>
              <given-names>H.M.</given-names>
            </name>
          </person-group>
          <article-title>Optimized Face Detector-Based Intelligent Face Mask Detection Model in IoT Using Deep Learning Approach</article-title>
          <source>Appl. Soft Comput.</source>
          <year>2023</year>
          <volume>134</volume>
          <fpage>109933</fpage>
          <pub-id pub-id-type="doi">10.1016/j.asoc.2022.109933</pub-id>
        </element-citation>
      </ref>
      <ref id="B24-electronics-14-00517">
        <label>24.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Lin</surname>
              <given-names>T.-Y.</given-names>
            </name>
            <name>
              <surname>Dollar</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Hariharan</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Belongie</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Feature Pyramid Networks for Object Detection</article-title>
          <source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <conf-loc>Honolulu, HI, USA</conf-loc>
          <conf-date>21&#x2013;26 July 2017</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2017</year>
          <fpage>936</fpage>
          <lpage>944</lpage>
        </element-citation>
      </ref>
      <ref id="B25-electronics-14-00517">
        <label>25.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Tan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Pang</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Le</surname>
              <given-names>Q.V.</given-names>
            </name>
          </person-group>
          <article-title>EfficientDet: Scalable and Efficient Object Detection</article-title>
          <source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <conf-loc>Seattle, WA, USA</conf-loc>
          <conf-date>13&#x2013;19 June 2020</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2020</year>
          <fpage>10778</fpage>
          <lpage>10787</lpage>
        </element-citation>
      </ref>
      <ref id="B26-electronics-14-00517">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Qi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>ACWFace: Efficient and Lightweight Face Detector Based on RetinaFace</article-title>
          <source>J. Electron. Imag.</source>
          <year>2022</year>
          <volume>31</volume>
          <fpage>013012</fpage>
          <pub-id pub-id-type="doi">10.1117/1.JEI.31.1.013012</pub-id>
        </element-citation>
      </ref>
      <ref id="B27-electronics-14-00517">
        <label>27.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Woo</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>J.-Y.</given-names>
            </name>
            <name>
              <surname>Kweon</surname>
              <given-names>I.S.</given-names>
            </name>
          </person-group>
          <article-title>CBAM: Convolutional Block Attention Module</article-title>
          <source>Computer Vision&#x2014;ECCV 2018</source>
          <person-group person-group-type="editor">
            <name>
              <surname>Ferrari</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Hebert</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Sminchisescu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Weiss</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <comment>Lecture Notes in Computer Science</comment>
          <publisher-name>Springer International Publishing</publisher-name>
          <publisher-loc>Cham, Switzerland</publisher-loc>
          <year>2018</year>
          <volume>Volume 11211</volume>
          <fpage>3</fpage>
          <lpage>19</lpage>
          <isbn>978-3-030-01233-5</isbn>
        </element-citation>
      </ref>
      <ref id="B28-electronics-14-00517">
        <label>28.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Squeeze-and-Excitation Networks</article-title>
          <source>Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <conf-loc>Salt Lake City, UT, USA</conf-loc>
          <conf-date>18&#x2013;23 June 2018</conf-date>
        </element-citation>
      </ref>
      <ref id="B29-electronics-14-00517">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>YOLO-FaceV2: A Scale and Occlusion Aware Face Detector</article-title>
          <source>Pattern Recognit.</source>
          <year>2024</year>
          <volume>155</volume>
          <fpage>110714</fpage>
          <pub-id pub-id-type="doi">10.1016/j.patcog.2024.110714</pub-id>
        </element-citation>
      </ref>
      <ref id="B30-electronics-14-00517">
        <label>30.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S.Z.</given-names>
            </name>
          </person-group>
          <article-title>FaceBoxes: A CPU Real-Time Face Detector with High Accuracy</article-title>
          <source>Proceedings of the 2017 IEEE International Joint Conference on Biometrics (IJCB)</source>
          <conf-loc>Denver, CO, USA</conf-loc>
          <conf-date>1&#x2013;4 October 2017</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <fpage>1</fpage>
          <lpage>9</lpage>
        </element-citation>
      </ref>
      <ref id="B31-electronics-14-00517">
        <label>31.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection</article-title>
          <source>Multimed. Syst.</source>
          <year>2023</year>
          <volume>29</volume>
          <fpage>2825</fpage>
          <lpage>2839</lpage>
          <pub-id pub-id-type="doi">10.1007/s00530-023-01134-6</pub-id>
        </element-citation>
      </ref>
      <ref id="B32-electronics-14-00517">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Jian</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Xiang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>LFFD: A Light and Fast Face Detector for Edge Devices</article-title>
          <source>arXiv</source>
          <year>2019</year>
          <pub-id pub-id-type="arxiv">1904.10633</pub-id>
        </element-citation>
      </ref>
      <ref id="B33-electronics-14-00517">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chao</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>A Lightweight Face Detector via Bi-Stream Convolutional Neural Network and Vision Transformer</article-title>
          <source>Information</source>
          <year>2024</year>
          <volume>15</volume>
          <elocation-id>290</elocation-id>
          <pub-id pub-id-type="doi">10.3390/info15050290</pub-id>
        </element-citation>
      </ref>
      <ref id="B34-electronics-14-00517">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>A Lightweight and Accurate Face Detection Algorithm Based on Retinaface</article-title>
          <source>arXiv</source>
          <year>2023</year>
          <pub-id pub-id-type="arxiv">2308.04340</pub-id>
        </element-citation>
      </ref>
      <ref id="B35-electronics-14-00517">
        <label>35.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Hasan</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Center and Scale Prediction: Anchor-Free Approach for Pedestrian and Face Detection</article-title>
          <source>Pattern Recognit.</source>
          <year>2023</year>
          <volume>135</volume>
          <fpage>109071</fpage>
          <pub-id pub-id-type="doi">10.1016/j.patcog.2022.109071</pub-id>
        </element-citation>
      </ref>
      <ref id="B36-electronics-14-00517">
        <label>36.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Qi</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Tan</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>YOLO5Face: Why Reinventing a Face Detector</article-title>
          <source>arXiv</source>
          <year>2022</year>
          <pub-id pub-id-type="arxiv">2105.12931</pub-id>
        </element-citation>
      </ref>
      <ref id="B37-electronics-14-00517">
        <label>37.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lattas</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Zafeiriou</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Sample and Computation Redistribution for Efficient Face Detection</article-title>
          <source>arXiv</source>
          <year>2021</year>
          <pub-id pub-id-type="arxiv">2105.04714</pub-id>
        </element-citation>
      </ref>
      <ref id="B38-electronics-14-00517">
        <label>38.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>YuNet: A Tiny Millisecond-Level Face Detector</article-title>
          <source>Mach. Intell. Res.</source>
          <year>2023</year>
          <volume>20</volume>
          <fpage>656</fpage>
          <lpage>665</lpage>
          <pub-id pub-id-type="doi">10.1007/s11633-023-1423-y</pub-id>
        </element-citation>
      </ref>
      <ref id="B39-electronics-14-00517">
        <label>39.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Dai</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Qi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Xiong</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Deformable Convolutional Networks</article-title>
          <source>Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</source>
          <conf-loc>Venice, Italy</conf-loc>
          <conf-date>22&#x2013;29 October 2017</conf-date>
        </element-citation>
      </ref>
      <ref id="B40-electronics-14-00517">
        <label>40.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Dai</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Deformable ConvNets V2: More Deformable, Better Results</article-title>
          <source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <conf-loc>Long Beach, CA, USA</conf-loc>
          <conf-date>15&#x2013;20 June 2019</conf-date>
        </element-citation>
      </ref>
      <ref id="B41-electronics-14-00517">
        <label>41.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Ma</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>H.-T.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</article-title>
          <source>Computer Vision&#x2014;ECCV 2018</source>
          <person-group person-group-type="editor">
            <name>
              <surname>Ferrari</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Hebert</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Sminchisescu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Weiss</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <comment>Lecture Notes in Computer Science</comment>
          <publisher-name>Springer International Publishing</publisher-name>
          <publisher-loc>Cham, Switzerland</publisher-loc>
          <year>2018</year>
          <volume>Volume 11218</volume>
          <fpage>122</fpage>
          <lpage>138</lpage>
          <isbn>978-3-030-01263-2</isbn>
        </element-citation>
      </ref>
      <ref id="B42-electronics-14-00517">
        <label>42.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Qiao</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks</article-title>
          <source>IEEE Signal Process. Lett.</source>
          <year>2016</year>
          <volume>23</volume>
          <fpage>1499</fpage>
          <lpage>1503</lpage>
          <pub-id pub-id-type="doi">10.1109/LSP.2016.2603342</pub-id>
        </element-citation>
      </ref>
    </ref-list>
    <sec sec-type="display-objects">
      <title>Figures and Tables</title>
      <fig id="electronics-14-00517-f001" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Overview of the FeatherFace architecture and its components. (<bold>a</bold>) FeatherFace integrates a MobileNet-0.25 backbone, attention mechanisms, multiscale feature aggregation, and detection heads. The integration of these modules jointly enhances feature representation, significantly improving the model&#x2019;s accuracy and robustness. (<bold>b</bold>) The convolutional block attention module (CBAM) applies both channel and spatial attention to refine features critical for accurate face detection. (<bold>c</bold>) The detection heads incorporate a context enhancement module, which uses deformable convolutional networks (DCNs) to capture multiscale contextual information, and a channel shuffling module to facilitate effective inter-channel information exchange, further enriching feature representation.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="electronics-14-00517-g001.tif"/>
      </fig>
      <fig id="electronics-14-00517-f002" position="float">
        <label>Figure 2</label>
        <caption>
          <p>(<bold>a</bold>) The evolution of the FeatherFace architecture via intermediate phases, illustrated with the overall AP achieved at each phase of development. The process consists of four sequential phases&#x2014;feature aggregation, attention mechanism, attention placement, and detection heads&#x2014;where the top-performing configuration from each phase was selected to guide the subsequent phase. The final FeatherFace architecture integrates a MobileNet-0.25 backbone, the bidirectional feature pyramid network (BiFPN) for multiscale feature aggregation, CBAM attention mechanisms applied to both the backbone and BiFPN output, and detection heads enhanced with DCNs and channel shuffling. The overall average precision (AP) represents the model&#x2019;s performance across the Easy, Medium, and Hard subsets of the WIDERFace dataset. Key architectures selected at each phase are highlighted in bold. Starred architectures denote the baseline (beginning) and final FeatherFace (end) architectures, marking the start and culmination of the optimization process. BB: backbone. (<bold>b</bold>) The overall precision&#x2013;recall curves comparing the baseline and three key architectures. Additional details, including phase-wise AP and precision&#x2013;recall curves for the Easy, Medium, and Hard subsets, are presented in <xref ref-type="app" rid="app1-electronics-14-00517">Figure S2</xref>.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="electronics-14-00517-g002.tif"/>
      </fig>
      <fig id="electronics-14-00517-f003" position="float">
        <label>Figure 3</label>
        <caption>
          <p>(<bold>a</bold>) Comparison of APs between the baseline and FeatherFace architectures across six challenging scenarios. Images were categorized based on the dominant challenge they presented, with a category assigned if a specific challenge-related facial box constituted more than 50% of the total facial boxes in the image (allowing for multiple category mappings). FeatherFace consistently outperformed the baseline across all scenarios, demonstrating superior detection accuracy under diverse and demanding conditions. (<bold>b</bold>) Representative images demonstrating the prediction results of the baseline and FeatherFace models for each scenario. Red boxes indicate the predicted facial boxes generated by each architecture.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="electronics-14-00517-g003.tif"/>
      </fig>
      <table-wrap id="electronics-14-00517-t001" position="float">
        <object-id pub-id-type="pii">electronics-14-00517-t001_Table 1</object-id>
        <label>Table 1</label>
        <caption>
          <p>The performance of the FeatherFace face-detection architecture on the WIDERFace validation set.</p>
        </caption>
        <table>
          <thead>
            <tr>
              <th colspan="6" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Architecture</th>
              <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">WIDERFace AP (%)</th>
            </tr>
            <tr>
              <th align="center" valign="middle" style="border-bottom:solid thin">Feature<break/>Aggregation</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Attention<break/>Mechanism</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Attention<break/>Placement</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Detection<break/>Heads</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Params</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">FLOPs</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Easy</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Medium</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Hard</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">FPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">-</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">-</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Conv</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">426.6 K</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">1.030 G</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">90.5</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">87.5</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">74.3</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">WFPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">-</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">-</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Conv</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">426.7 K</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">1.030 G</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">88.6</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">85.9</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">73.7</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">BiFPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">-</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">-</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Conv</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">436.1 K</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">900.8 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">90.9</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">88.3</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">74.6</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">BiFPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">SENet</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Backbone + BiFPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Conv</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">448.4 K</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">902.1 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">92.0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">89.5</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">76.4</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">BiFPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">CBAM</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Backbone + BiFPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Conv</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">449.7 K</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">902.5 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">92.2</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">89.8</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">76.6</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">BiFPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">CBAM</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Backbone</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Conv</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">447.7 K</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">901.7 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">91.6</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">88.8</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">75.4</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">BiFPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">CBAM</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Backbone + BiFPN + SSH</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Conv</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">451.8 K</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">903.4 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">90.6</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">88.9</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">75.1</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">BiFPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">CBAM</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Backbone + BiFPN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">DCN</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">474.7 K</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">971.5 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">92.3</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">90.0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">77.5</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>BiFPN</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>CBAM</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>Backbone + BiFPN</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>DCN + shuffle *</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>488.7 K</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>1.013 G</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>92.7</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>90.7</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>78.3</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p>* FeatherFace architecture.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap id="electronics-14-00517-t002" position="float">
        <object-id pub-id-type="pii">electronics-14-00517-t002_Table 2</object-id>
        <label>Table 2</label>
        <caption>
          <p>The ablation study results for the FeatherFace architecture, analyzing the impacts of different multiscale feature input configurations and the number of BiFPN layers.</p>
        </caption>
        <table>
          <thead>
            <tr>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Feature Inputs</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Number of BiFPN Layers</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Params</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">FLOPs</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Easy</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Medium</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Hard</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" valign="middle">P3</td>
              <td align="center" valign="middle">0</td>
              <td align="center" valign="middle">60.62 K</td>
              <td align="center" valign="middle">489.4 M</td>
              <td align="center" valign="middle">23.0</td>
              <td align="center" valign="middle">55.0</td>
              <td align="center" valign="middle">60.4</td>
            </tr>
            <tr>
              <td align="center" valign="middle">P4</td>
              <td align="center" valign="middle">0</td>
              <td align="center" valign="middle">192.7 K</td>
              <td align="center" valign="middle">462.0 M</td>
              <td align="center" valign="middle">29.9</td>
              <td align="center" valign="middle">59.1</td>
              <td align="center" valign="middle">58.9</td>
            </tr>
            <tr>
              <td align="center" valign="middle">P5</td>
              <td align="center" valign="middle">0</td>
              <td align="center" valign="middle">364.8 K</td>
              <td align="center" valign="middle">431.4 M</td>
              <td align="center" valign="middle">40.6</td>
              <td align="center" valign="middle">61.5</td>
              <td align="center" valign="middle">46.9</td>
            </tr>
            <tr>
              <td align="center" valign="middle">P3, P4</td>
              <td align="center" valign="middle">3</td>
              <td align="center" valign="middle">263.7 K</td>
              <td align="center" valign="middle">907.4 M</td>
              <td align="center" valign="middle">85.6</td>
              <td align="center" valign="middle">86.6</td>
              <td align="center" valign="middle">75.9</td>
            </tr>
            <tr>
              <td align="center" valign="middle">P4, P5</td>
              <td align="center" valign="middle">3</td>
              <td align="center" valign="middle">395.6 K</td>
              <td align="center" valign="middle">531.0 M</td>
              <td align="center" valign="middle">80.6</td>
              <td align="center" valign="middle">82.3</td>
              <td align="center" valign="middle">68.2</td>
            </tr>
            <tr>
              <td align="center" valign="middle">P3, P5</td>
              <td align="center" valign="middle">3</td>
              <td align="center" valign="middle">389.9 K</td>
              <td align="center" valign="middle">857.8 M</td>
              <td align="center" valign="middle">83.8</td>
              <td align="center" valign="middle">86.4</td>
              <td align="center" valign="middle">77.0</td>
            </tr>
            <tr>
              <td align="center" valign="middle">P3, P4, P5</td>
              <td align="center" valign="middle">1</td>
              <td align="center" valign="middle">449.7 K</td>
              <td align="center" valign="middle">914.0 M</td>
              <td align="center" valign="middle">92.5</td>
              <td align="center" valign="middle">90.3</td>
              <td align="center" valign="middle">77.8</td>
            </tr>
            <tr>
              <td align="center" valign="middle">P3, P4, P5</td>
              <td align="center" valign="middle">2</td>
              <td align="center" valign="middle">469.2 K</td>
              <td align="center" valign="middle">963.3 M</td>
              <td align="center" valign="middle">92.8</td>
              <td align="center" valign="middle">90.4</td>
              <td align="center" valign="middle">78.1</td>
            </tr>
            <tr>
              <td align="center" valign="middle">
                <bold>P3, P4, P5</bold>
              </td>
              <td align="center" valign="middle">
                <bold>3 *</bold>
              </td>
              <td align="center" valign="middle">
                <bold>488.7 K</bold>
              </td>
              <td align="center" valign="middle">
                <bold>1.013 G</bold>
              </td>
              <td align="center" valign="middle">
                <bold>92.7</bold>
              </td>
              <td align="center" valign="middle">
                <bold>90.7</bold>
              </td>
              <td align="center" valign="middle">
                <bold>78.3</bold>
              </td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">P3, P4, P5</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">4</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">508.1 K</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">1.062 G</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">92.7</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">90.4</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">77.9</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p>* FeatherFace architecture.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap id="electronics-14-00517-t003" position="float">
        <object-id pub-id-type="pii">electronics-14-00517-t003_Table 3</object-id>
        <label>Table 3</label>
        <caption>
          <p>Comparative performance of FeatherFace and existing lightweight face detectors on WIDERFace dataset. Metrics include AP on Easy, Medium, and Hard subsets, along with parameter counts and floating-point operations (FLOPs).</p>
        </caption>
        <table>
          <thead>
            <tr>
              <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Architecture</th>
              <th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">WIDERFace AP (%)</th>
            </tr>
            <tr>
              <th align="center" valign="middle" style="border-bottom:solid thin">Face Detector</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Params</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">FLOPs</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Easy</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Medium</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Hard</th>
              <th align="center" valign="middle" style="border-bottom:solid thin">Overall</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">RetinaFace [<xref ref-type="bibr" rid="B9-electronics-14-00517">9</xref>]</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">25.1 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">32.2 G</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">95.6</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">94.2</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">86.3</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">92.0</td>
            </tr>
            <tr>
              <td align="center" valign="middle">YOLOv5 s [<xref ref-type="bibr" rid="B36-electronics-14-00517">36</xref>]</td>
              <td align="center" valign="middle">7.08 M</td>
              <td align="center" valign="middle">5.75 G</td>
              <td align="center" valign="middle">94.3</td>
              <td align="center" valign="middle">92.6</td>
              <td align="center" valign="middle">83.2</td>
              <td align="center" valign="middle">90.0</td>
            </tr>
            <tr>
              <td align="center" valign="middle">SCRFD-10 GF [<xref ref-type="bibr" rid="B37-electronics-14-00517">37</xref>]</td>
              <td align="center" valign="middle">3.86 M</td>
              <td align="center" valign="middle">9.98 G</td>
              <td align="center" valign="middle">95.2</td>
              <td align="center" valign="middle">93.9</td>
              <td align="center" valign="middle">83.1</td>
              <td align="center" valign="middle">90.7</td>
            </tr>
            <tr>
              <td align="center" valign="middle">LFFDv2 [<xref ref-type="bibr" rid="B32-electronics-14-00517">32</xref>]</td>
              <td align="center" valign="middle">1.45 M</td>
              <td align="center" valign="middle">6.87 G</td>
              <td align="center" valign="middle">83.7</td>
              <td align="center" valign="middle">83.5</td>
              <td align="center" valign="middle">72.9</td>
              <td align="center" valign="middle">80.0</td>
            </tr>
            <tr>
              <td align="center" valign="middle">YOLOv5n [<xref ref-type="bibr" rid="B36-electronics-14-00517">36</xref>]</td>
              <td align="center" valign="middle">1.73 M</td>
              <td align="center" valign="middle">2.11 G</td>
              <td align="center" valign="middle">93.6</td>
              <td align="center" valign="middle">91.5</td>
              <td align="center" valign="middle">80.5</td>
              <td align="center" valign="middle">88.5</td>
            </tr>
            <tr>
              <td align="center" valign="middle">FaceBoxes [<xref ref-type="bibr" rid="B30-electronics-14-00517">30</xref>]</td>
              <td align="center" valign="middle">1.01 M</td>
              <td align="center" valign="middle">0.275 G</td>
              <td align="center" valign="middle">76.2</td>
              <td align="center" valign="middle">57.2</td>
              <td align="center" valign="middle">24.2</td>
              <td align="center" valign="middle">52.5</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-top:solid thin">SCRFD-0.5GF [<xref ref-type="bibr" rid="B37-electronics-14-00517">37</xref>]</td>
              <td align="center" valign="middle" style="border-top:solid thin">0.57 M</td>
              <td align="center" valign="middle" style="border-top:solid thin">0.508 G</td>
              <td align="center" valign="middle" style="border-top:solid thin">90.6</td>
              <td align="center" valign="middle" style="border-top:solid thin">88.1</td>
              <td align="center" valign="middle" style="border-top:solid thin">68.5</td>
              <td align="center" valign="middle" style="border-top:solid thin">82.4</td>
            </tr>
            <tr>
              <td align="center" valign="middle">MTCNN [<xref ref-type="bibr" rid="B42-electronics-14-00517">42</xref>]</td>
              <td align="center" valign="middle">0.50 M</td>
              <td align="center" valign="middle">4.6 G</td>
              <td align="center" valign="middle">85.1</td>
              <td align="center" valign="middle">82.0</td>
              <td align="center" valign="middle">60.7</td>
              <td align="center" valign="middle">75.9</td>
            </tr>
            <tr>
              <td align="center" valign="middle">YOLOv5n0.5 [<xref ref-type="bibr" rid="B36-electronics-14-00517">36</xref>]</td>
              <td align="center" valign="middle">0.447 M</td>
              <td align="center" valign="middle">0.571 G</td>
              <td align="center" valign="middle">90.8</td>
              <td align="center" valign="middle">88.1</td>
              <td align="center" valign="middle">73.8</td>
              <td align="center" valign="middle">84.2</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>FeatherFace (ours)</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>0.489 M</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>1.01 G</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>92.7</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>90.7</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>78.3</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>87.2</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap id="electronics-14-00517-t004" position="float">
        <object-id pub-id-type="pii">electronics-14-00517-t004_Table 4</object-id>
        <label>Table 4</label>
        <caption>
          <p>Effectiveness of FeatherFace Core when applied to various backbone networks.</p>
        </caption>
        <table>
          <thead>
            <tr>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Backbone</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Network Configuration</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Params</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">FLOPs</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Easy</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Medium</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Hard</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Overall</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin">CSPDarknet53</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Baseline (RetinaFace core)</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">29.9 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">51.0 G</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">94.7</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">93.5</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">87.1</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">91.8</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>+FeatherFace Core</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>29.2 M</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>46.1 G</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>95.2</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>93.9</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>88.0</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>92.3</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin">EfficientNet-B0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Baseline (RetinaFace core)</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">3.88 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">3.96 G</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">93.0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">91.5</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">83.9</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">89.5</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>+FeatherFace Core</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>3.93 M</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>3.81 G</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>9</bold>
                <bold>3.5</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>92.0</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>84.5</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>90.0</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin">MnasNet-0.5</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Baseline (RetinaFace core)</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">0.607 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">0.893 G</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">69.2</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">71.2</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">48.6</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">63.0</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>+FeatherFace Core</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>0.649 M</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>0.933 G</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>91.9</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>89.8</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>76.9</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>86.2</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin">ShuffleNet-V2-0.25</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Baseline (RetinaFace core)</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">0.265 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">0.649 G</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">90.7</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">87.6</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">72.6</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">83.6</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>+FeatherFace Core</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>0.326 M</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>0.713 G</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>92.2</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>89.9</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>7</bold>
                <bold>5.9</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>86.0</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin">MobileNet-0.25</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">Baseline (RetinaFace core)</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">0.427 M</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">1.03 G</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">90.5</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">87.5</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">74.3</td>
              <td align="center" valign="middle" style="border-bottom:solid thin">84.1</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>+FeatherFace Core</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>0.489 M</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>1.01 G</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>92.7</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>90.7</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>78.3</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin">
                <bold>87.2</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <fn-group>
      <fn>
        <p><bold>Disclaimer/Publisher&#x2019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p>
      </fn>
    </fn-group>
  </back>
</article>
