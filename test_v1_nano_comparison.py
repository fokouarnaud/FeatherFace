#!/usr/bin/env python3
"""
FeatherFace V1 vs Nano Comparison Script
Comprehensive comparison of FeatherFace V1 baseline vs Nano ultra-efficient model

Scientific Foundation:
- V1: Original FeatherFace architecture (487K parameters)
- Nano: Scientifically justified efficient architecture (344K parameters, 29% reduction)
"""

import torch
import torch.nn as nn
import time
import numpy as np
import cv2
import argparse
import os
from typing import Dict, List, Tuple

from models.retinaface import RetinaFace
from models.featherface_nano import FeatherFaceNano
from data.config import cfg_mnet, cfg_nano
from layers.functions.prior_box import PriorBox
from utils.nms.py_cpu_nms import py_cpu_nms
from utils.box_utils import decode, decode_landm


def parse_args():
    parser = argparse.ArgumentParser(description='FeatherFace V1 vs Nano Comparison')
    parser.add_argument('--v1_model', default='./weights/mobilenet0.25_Final.pth',
                        help='Path to V1 model weights')
    parser.add_argument('--nano_model', default='./weights/nano/nano_final.pth',
                        help='Path to Nano model weights')
    parser.add_argument('--test_images', default='./test_images/',
                        help='Directory containing test images')
    parser.add_argument('--confidence_threshold', default=0.02, type=float,
                        help='Confidence threshold for detection')
    parser.add_argument('--nms_threshold', default=0.4, type=float,
                        help='NMS threshold')
    parser.add_argument('--vis_threshold', default=0.6, type=float,
                        help='Visualization threshold')
    parser.add_argument('--output_dir', default='./comparison_results/',
                        help='Output directory for comparison results')
    parser.add_argument('--benchmark_runs', default=100, type=int,
                        help='Number of runs for speed benchmark')
    
    return parser.parse_args()


class ModelComparator:
    """Comprehensive model comparison tool"""
    
    def __init__(self, v1_model_path: str, nano_model_path: str, device: str = 'cuda'):
        self.device = device
        
        # Load V1 model
        print("ğŸ“Š Loading FeatherFace V1 model...")
        self.v1_model = RetinaFace(cfg=cfg_mnet, phase='test')
        v1_checkpoint = torch.load(v1_model_path, map_location='cpu')
        self.v1_model.load_state_dict(v1_checkpoint, strict=False)
        self.v1_model = self.v1_model.to(device)
        self.v1_model.eval()
        
        # Load Nano model  
        print("ğŸ“Š Loading FeatherFace Nano model...")
        self.nano_model = FeatherFaceNano(cfg=cfg_nano, phase='test')
        nano_checkpoint = torch.load(nano_model_path, map_location='cpu')
        
        # Handle checkpoint format
        if 'model_state_dict' in nano_checkpoint:
            self.nano_model.load_state_dict(nano_checkpoint['model_state_dict'], strict=False)
        else:
            self.nano_model.load_state_dict(nano_checkpoint, strict=False)
            
        self.nano_model = self.nano_model.to(device)
        self.nano_model.eval()
        
        # Parameter analysis
        self.v1_params = sum(p.numel() for p in self.v1_model.parameters())
        self.nano_params = sum(p.numel() for p in self.nano_model.parameters())
        
        print(f"âœ… Models loaded successfully!")
        print(f"ğŸ“Š V1 Parameters: {self.v1_params:,}")
        print(f"ğŸ“Š Nano Parameters: {self.nano_params:,}")
        print(f"ğŸ“Š Parameter Reduction: {((self.v1_params - self.nano_params) / self.v1_params * 100):.1f}%")
    
    def count_parameters_detailed(self) -> Dict[str, Dict[str, int]]:
        """Detailed parameter breakdown comparison"""
        
        # V1 breakdown (simplified)
        v1_breakdown = {
            'backbone': sum(p.numel() for p in self.v1_model.body.parameters()),
            'fpn': sum(p.numel() for p in self.v1_model.fpn.parameters()) if hasattr(self.v1_model, 'fpn') else 0,
            'ssh': sum(p.numel() for p in [self.v1_model.ssh1, self.v1_model.ssh2, self.v1_model.ssh3]),
            'head': sum(p.numel() for p in [self.v1_model.ClassHead, self.v1_model.BboxHead, self.v1_model.LandmarkHead]),
            'total': self.v1_params
        }
        
        # Nano breakdown (detailed)
        from models.featherface_nano import count_parameters_detailed
        nano_breakdown = count_parameters_detailed(self.nano_model)
        
        return {'v1': v1_breakdown, 'nano': nano_breakdown}
    
    def benchmark_speed(self, input_size: Tuple[int, int] = (640, 640), num_runs: int = 100) -> Dict[str, float]:
        """Benchmark inference speed"""
        
        print(f"â±ï¸ Benchmarking inference speed ({num_runs} runs)...")
        
        # Create dummy input
        dummy_input = torch.randn(1, 3, input_size[0], input_size[1]).to(self.device)
        
        # Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = self.v1_model(dummy_input)
                _ = self.nano_model(dummy_input)
        
        # Benchmark V1
        torch.cuda.synchronize()
        v1_times = []
        
        for _ in range(num_runs):
            start_time = time.time()
            with torch.no_grad():
                _ = self.v1_model(dummy_input)
            torch.cuda.synchronize()
            v1_times.append(time.time() - start_time)
        
        # Benchmark Nano
        torch.cuda.synchronize()
        nano_times = []
        
        for _ in range(num_runs):
            start_time = time.time()
            with torch.no_grad():
                _ = self.nano_model(dummy_input)
            torch.cuda.synchronize()
            nano_times.append(time.time() - start_time)
        
        # Calculate statistics
        v1_avg = np.mean(v1_times) * 1000  # Convert to ms
        nano_avg = np.mean(nano_times) * 1000
        
        speedup = v1_avg / nano_avg
        
        return {
            'v1_avg_ms': v1_avg,
            'v1_fps': 1000 / v1_avg,
            'nano_avg_ms': nano_avg,
            'nano_fps': 1000 / nano_avg,
            'speedup': speedup
        }
    
    def detect_faces(self, image: np.ndarray, model_type: str = 'nano', 
                     conf_threshold: float = 0.02, nms_threshold: float = 0.4) -> List[Dict]:
        """Detect faces using specified model"""
        
        if model_type == 'v1':
            model = self.v1_model
            cfg = cfg_mnet
        else:
            model = self.nano_model
            cfg = cfg_nano
        
        # Preprocess image
        img = np.float32(image)
        img -= (104, 117, 123)
        img = img.transpose(2, 0, 1)
        img = torch.from_numpy(img).unsqueeze(0).to(self.device)
        
        # Inference
        with torch.no_grad():
            loc, conf, landms = model(img)
        
        # Post-process
        priorbox = PriorBox(cfg, image_size=(img.shape[2], img.shape[3]))
        priors = priorbox.forward()
        priors = priors.to(self.device)
        prior_data = priors.data
        
        boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])
        boxes = boxes * torch.tensor([image.shape[1], image.shape[0], image.shape[1], image.shape[0]]).to(self.device)
        boxes = boxes.cpu().numpy()
        
        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]
        landms_pred = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])
        landms_pred = landms_pred * torch.tensor([image.shape[1], image.shape[0]] * 5).to(self.device)
        landms_pred = landms_pred.cpu().numpy()
        
        # Filter by confidence
        inds = np.where(scores > conf_threshold)[0]
        boxes = boxes[inds]
        landms_pred = landms_pred[inds]
        scores = scores[inds]
        
        # NMS
        order = scores.argsort()[::-1]
        boxes = boxes[order]
        landms_pred = landms_pred[order]
        scores = scores[order]
        
        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)
        keep = py_cpu_nms(dets, nms_threshold)
        
        dets = dets[keep, :]
        landms_pred = landms_pred[keep]
        
        # Format results
        results = []
        for i in range(dets.shape[0]):
            results.append({
                'bbox': dets[i, :4],
                'confidence': dets[i, 4],
                'landmarks': landms_pred[i].reshape(5, 2)
            })
        
        return results
    
    def compare_detection_results(self, image_path: str, vis_threshold: float = 0.6) -> Dict:
        """Compare detection results between V1 and Nano"""
        
        # Load image
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Could not load image: {image_path}")
        
        # Detect with both models
        v1_results = self.detect_faces(image, 'v1')
        nano_results = self.detect_faces(image, 'nano')
        
        # Filter by visualization threshold
        v1_results = [r for r in v1_results if r['confidence'] > vis_threshold]
        nano_results = [r for r in nano_results if r['confidence'] > vis_threshold]
        
        return {
            'image_path': image_path,
            'v1_detections': len(v1_results),
            'nano_detections': len(nano_results),
            'v1_results': v1_results,
            'nano_results': nano_results
        }
    
    def generate_comparison_report(self, test_images_dir: str, output_dir: str, 
                                 benchmark_runs: int = 100) -> Dict:
        """Generate comprehensive comparison report"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        print("\nğŸ”¬ Generating FeatherFace V1 vs Nano Comparison Report")
        print("=" * 60)
        
        # 1. Parameter Analysis
        print("\nğŸ“Š Parameter Analysis:")
        param_breakdown = self.count_parameters_detailed()
        
        print(f"  V1 Total: {param_breakdown['v1']['total']:,} parameters")
        print(f"  Nano Total: {param_breakdown['nano']['total']:,} parameters")
        print(f"  Reduction: {((param_breakdown['v1']['total'] - param_breakdown['nano']['total']) / param_breakdown['v1']['total'] * 100):.1f}%")
        
        # 2. Speed Benchmark
        print("\nâ±ï¸ Speed Benchmark:")
        speed_results = self.benchmark_speed(num_runs=benchmark_runs)
        
        print(f"  V1: {speed_results['v1_avg_ms']:.2f}ms ({speed_results['v1_fps']:.1f} FPS)")
        print(f"  Nano: {speed_results['nano_avg_ms']:.2f}ms ({speed_results['nano_fps']:.1f} FPS)")
        print(f"  Speedup: {speed_results['speedup']:.2f}x")
        
        # 3. Detection Comparison
        detection_results = []
        if os.path.exists(test_images_dir):
            print(f"\nğŸ¯ Detection Comparison on test images:")
            image_files = [f for f in os.listdir(test_images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            
            for img_file in image_files[:10]:  # Test on first 10 images
                img_path = os.path.join(test_images_dir, img_file)
                try:
                    result = self.compare_detection_results(img_path)
                    detection_results.append(result)
                    print(f"  {img_file}: V1={result['v1_detections']}, Nano={result['nano_detections']}")
                except Exception as e:
                    print(f"  {img_file}: Error - {e}")
        
        # 4. Generate Report
        report = {
            'models': {
                'v1': {
                    'parameters': param_breakdown['v1']['total'],
                    'avg_inference_ms': speed_results['v1_avg_ms'],
                    'fps': speed_results['v1_fps']
                },
                'nano': {
                    'parameters': param_breakdown['nano']['total'],
                    'avg_inference_ms': speed_results['nano_avg_ms'],
                    'fps': speed_results['nano_fps']
                }
            },
            'comparison': {
                'parameter_reduction_percent': ((param_breakdown['v1']['total'] - param_breakdown['nano']['total']) / param_breakdown['v1']['total'] * 100),
                'speedup': speed_results['speedup'],
                'efficiency_gain': speed_results['speedup'] * (param_breakdown['v1']['total'] / param_breakdown['nano']['total'])
            },
            'detection_results': detection_results,
            'scientific_foundation': {
                'v1': 'Original FeatherFace architecture',
                'nano': 'Li et al. CVPR 2023 (Knowledge Distillation) + Woo et al. ECCV 2018 (CBAM) + Tan et al. CVPR 2020 (BiFPN)'
            }
        }
        
        # Save report
        import json
        report_path = os.path.join(output_dir, 'v1_nano_comparison_report.json')
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nâœ… Comparison report saved: {report_path}")
        
        return report


def main():
    args = parse_args()
    
    print("ğŸ”¬ FeatherFace V1 vs Nano Comparison Tool")
    print("=" * 50)
    print("ğŸ“Š V1: Original FeatherFace (487K parameters)")
    print("ğŸ“Š Nano: Scientifically justified ultra-efficient (344K parameters)")
    print("ğŸ”¬ Scientific basis: Knowledge distillation + proven efficiency techniques")
    
    # Initialize comparator
    try:
        comparator = ModelComparator(args.v1_model, args.nano_model)
    except Exception as e:
        print(f"âŒ Error loading models: {e}")
        return
    
    # Generate comparison report
    try:
        report = comparator.generate_comparison_report(
            args.test_images, 
            args.output_dir,
            args.benchmark_runs
        )
        
        print(f"\nğŸ† FeatherFace Nano Efficiency Summary:")
        print(f"  ğŸ“‰ Parameter Reduction: {report['comparison']['parameter_reduction_percent']:.1f}%")
        print(f"  âš¡ Speed Improvement: {report['comparison']['speedup']:.2f}x")
        print(f"  ğŸ¯ Overall Efficiency Gain: {report['comparison']['efficiency_gain']:.2f}x")
        print(f"  ğŸ”¬ Scientific Foundation: 4 verified research papers")
        
    except Exception as e:
        print(f"âŒ Error during comparison: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()