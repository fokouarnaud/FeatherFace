{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatherFace Baseline Training and Evaluation\n",
    "\n",
    "This notebook reproduces the original FeatherFace training process following the author's instructions.\n",
    "\n",
    "## Overview\n",
    "- Model: FeatherFace with MobileNetV1 0.25x backbone\n",
    "- Dataset: WIDERFace (auto-download)\n",
    "- Expected Results: 0.49M parameters, 90.8% mAP\n",
    "- Uses original training scripts for faithful reproduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -e .\n",
    "!pip install gdown requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify imports and check GPU\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import gdown\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automatic Dataset Download - WIDERFace\n",
    "\n",
    "Download and organize the WIDERFace dataset automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directories\n",
    "data_root = Path('./data/widerface')\n",
    "data_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# WIDERFace download links\n",
    "WIDERFACE_GDRIVE_ID = '11UGV3nbVv1x9IC--_tK3Uxf7hA6rlbsS'\n",
    "WIDERFACE_URL = f'https://drive.google.com/uc?id={WIDERFACE_GDRIVE_ID}'\n",
    "\n",
    "def download_widerface():\n",
    "    \"\"\"Download WIDERFace dataset from Google Drive\"\"\"\n",
    "    output_path = data_root / 'widerface.zip'\n",
    "    \n",
    "    if not output_path.exists():\n",
    "        print(\"Downloading WIDERFace dataset...\")\n",
    "        print(\"This may take several minutes depending on your connection.\")\n",
    "        \n",
    "        try:\n",
    "            gdown.download(WIDERFACE_URL, str(output_path), quiet=False)\n",
    "            print(f\"✓ Downloaded to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Download failed: {e}\")\n",
    "            print(\"Please download manually from:\")\n",
    "            print(f\"  {WIDERFACE_URL}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"✓ Dataset already downloaded: {output_path}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Download dataset\n",
    "if download_widerface():\n",
    "    print(\"\\n✅ Dataset download complete!\")\n",
    "else:\n",
    "    print(\"\\n❌ Please download the dataset manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset\n",
    "def extract_widerface():\n",
    "    \"\"\"Extract WIDERFace dataset\"\"\"\n",
    "    zip_path = data_root / 'widerface.zip'\n",
    "    \n",
    "    if not zip_path.exists():\n",
    "        print(\"❌ Dataset zip file not found. Please download first.\")\n",
    "        return False\n",
    "    \n",
    "    # Check if already extracted\n",
    "    if (data_root / 'train' / 'label.txt').exists() and \\\n",
    "       (data_root / 'val' / 'wider_val.txt').exists():\n",
    "        print(\"✓ Dataset already extracted\")\n",
    "        return True\n",
    "    \n",
    "    print(\"Extracting dataset...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_root)\n",
    "        print(\"✓ Dataset extracted successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Extraction failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Extract dataset\n",
    "if extract_widerface():\n",
    "    print(\"\\n✅ Dataset ready for use!\")\n",
    "else:\n",
    "    print(\"\\n❌ Please extract the dataset manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "def verify_dataset():\n",
    "    \"\"\"Verify WIDERFace dataset structure\"\"\"\n",
    "    required_files = [\n",
    "        data_root / 'train' / 'label.txt',\n",
    "        data_root / 'val' / 'wider_val.txt'\n",
    "    ]\n",
    "    \n",
    "    all_present = True\n",
    "    for file_path in required_files:\n",
    "        if file_path.exists():\n",
    "            print(f\"✓ Found: {file_path}\")\n",
    "        else:\n",
    "            print(f\"✗ Missing: {file_path}\")\n",
    "            all_present = False\n",
    "    \n",
    "    # Check for images\n",
    "    for split in ['train', 'val']:\n",
    "        img_dir = data_root / split / 'images'\n",
    "        if img_dir.exists():\n",
    "            img_count = len(list(img_dir.glob('**/*.jpg')))\n",
    "            print(f\"✓ {split} images: {img_count} found\")\n",
    "        else:\n",
    "            print(f\"✗ {split} images directory not found\")\n",
    "            all_present = False\n",
    "    \n",
    "    return all_present\n",
    "\n",
    "dataset_ready = verify_dataset()\n",
    "print(f\"\\nDataset verification: {'PASSED ✅' if dataset_ready else 'FAILED ❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic Pre-trained Weights Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights directory\n",
    "weights_dir = Path('./weights')\n",
    "weights_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Pre-trained weights info\n",
    "PRETRAIN_GDRIVE_ID = '1oZRSG0ZegbVkVwUd8wUIQx8W7yfZ_ki1'\n",
    "PRETRAIN_URL = f'https://drive.google.com/uc?id={PRETRAIN_GDRIVE_ID}'\n",
    "PRETRAIN_FILENAME = 'mobilenetV1X0.25_pretrain.tar'\n",
    "\n",
    "def download_pretrained_weights():\n",
    "    \"\"\"Download pre-trained MobileNetV1 0.25x weights\"\"\"\n",
    "    output_path = weights_dir / PRETRAIN_FILENAME\n",
    "    \n",
    "    if not output_path.exists():\n",
    "        print(\"Downloading pre-trained weights...\")\n",
    "        try:\n",
    "            gdown.download(PRETRAIN_URL, str(output_path), quiet=False)\n",
    "            print(f\"✓ Downloaded to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Download failed: {e}\")\n",
    "            print(\"Please download manually from:\")\n",
    "            print(f\"  {PRETRAIN_URL}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"✓ Pre-trained weights already exist: {output_path}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Download weights\n",
    "if download_pretrained_weights():\n",
    "    print(\"\\n✅ Pre-trained weights ready!\")\n",
    "else:\n",
    "    print(\"\\n❌ Please download weights manually.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration Check\n",
    "\n",
    "Following the author's instructions to check network configuration in `data/config.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration\n",
    "from data import cfg_mnet\n",
    "\n",
    "# Display configuration as mentioned in README\n",
    "print(\"=== FeatherFace Configuration (cfg_mnet) ===\")\n",
    "print(f\"Network name: {cfg_mnet['name']}\")\n",
    "print(f\"Input image size: {cfg_mnet['image_size']}\")\n",
    "print(f\"Batch size: {cfg_mnet['batch_size']}\")\n",
    "print(f\"Learning rate: {cfg_mnet['lr']}\")\n",
    "print(f\"Training epochs: {cfg_mnet['epoch']}\")\n",
    "print(f\"GPU train: {cfg_mnet['gpu_train']}\")\n",
    "print(f\"\\nMin sizes: {cfg_mnet['min_sizes']}\")\n",
    "print(f\"Steps: {cfg_mnet['steps']}\")\n",
    "print(f\"Variance: {cfg_mnet['variance']}\")\n",
    "print(f\"Clip: {cfg_mnet['clip']}\")\n",
    "print(f\"Loc weight: {cfg_mnet['loc_weight']}\")\n",
    "print(f\"\\nNote: You can modify these in data/config.py before training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Using Original Scripts\n",
    "\n",
    "Following the author's command: `CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 train.py --network mobile0.25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Train using the author's script (recommended for full training)\n",
    "print(\"To train the model, run this command in terminal:\")\n",
    "print(\"\\nCUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 train.py --network mobile0.25\")\n",
    "print(\"\\nOr run the cell below to execute from notebook:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training script from notebook\n",
    "# Uncomment to run full training (will take several hours)\n",
    "\n",
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# cmd = ['torchrun', '--standalone', '--nproc_per_node=1', '../train.py', '--network', 'mobile0.25']\n",
    "# subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Training Demo (Alternative)\n",
    "\n",
    "For demonstration purposes, here's a simplified training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for demo\n",
    "from models.retinaface import RetinaFace\n",
    "from layers.modules import MultiBoxLoss\n",
    "from data import WiderFaceDetection, detection_collate\n",
    "import data.data_augment as data_augment\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Model setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "net = RetinaFace(cfg=cfg_mnet)\n",
    "print(f\"Model created: {cfg_mnet['name']}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "\n",
    "# Load pre-trained backbone\n",
    "pretrain_path = weights_dir / PRETRAIN_FILENAME\n",
    "if pretrain_path.exists():\n",
    "    print(f\"\\nLoading pre-trained weights from {pretrain_path}\")\n",
    "    state_dict = torch.load(str(pretrain_path), map_location=device)\n",
    "    # Load only matching keys\n",
    "    model_dict = net.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    net.load_state_dict(model_dict)\n",
    "    print(f\"Loaded {len(pretrained_dict)} pre-trained layers\")\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training demo (2 epochs only)\n",
    "def train_demo(net, num_epochs=2):\n",
    "    \"\"\"Demo training for notebook - use train.py for full training\"\"\"\n",
    "    # Data setup\n",
    "    rgb_mean = (104, 117, 123)\n",
    "    train_dataset = WiderFaceDetection(\n",
    "        str(data_root / 'train/label.txt'),\n",
    "        data_augment.preproc(cfg_mnet['image_size'], rgb_mean)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg_mnet['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        collate_fn=detection_collate,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = MultiBoxLoss(cfg_mnet['num_classes'], 0.35, True, 0, True, 7, 0.35, False)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=cfg_mnet['lr'], momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"\\nStarting demo training for {num_epochs} epochs...\")\n",
    "    \n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            if batch_idx >= 5:  # Only 5 batches for demo\n",
    "                break\n",
    "                \n",
    "            images = images.to(device)\n",
    "            targets = [anno.to(device) for anno in targets]\n",
    "            \n",
    "            out = net(images)\n",
    "            loss_l, loss_c, loss_landm = criterion(out, targets)\n",
    "            loss = cfg_mnet['loc_weight'] * loss_l + loss_c + loss_landm\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 2 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx}/{min(5, len(train_loader))}] \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"\\n✅ Demo training complete!\")\n",
    "    print(\"Note: For full training, use train.py script as shown above.\")\n",
    "\n",
    "# Uncomment to run demo\n",
    "# train_demo(net, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Using Original Scripts\n",
    "\n",
    "Following the author's evaluation process:\n",
    "1. Generate txt files using `test_widerface.py`\n",
    "2. Evaluate using WIDERFace evaluation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate detection results\n",
    "print(\"=== Evaluation Step 1: Generate Detection Results ===\")\n",
    "print(\"\\nRun this command after training completes:\")\n",
    "print(\"python test_widerface.py --trained_model ./weights/mobilenet0.25_Final.pth --network mobile0.25 --origin_size True\")\n",
    "print(\"\\nThis will generate detection results in ./widerface_txt/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Evaluate results\n",
    "print(\"=== Evaluation Step 2: Calculate mAP ===\")\n",
    "print(\"\\nRun these commands after generating detection results:\")\n",
    "print(\"cd ./widerface_evaluate\")\n",
    "print(\"python evaluation.py -p ./widerface_txt -g ./eval_tools/ground_truth\")\n",
    "print(\"\\nThis will output the mAP scores for Easy, Medium, and Hard subsets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test function for single image\n",
    "def test_single_image(image_path, model_path=None):\n",
    "    \"\"\"Test detection on a single image\"\"\"\n",
    "    # Load model if path provided\n",
    "    if model_path and Path(model_path).exists():\n",
    "        net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    \n",
    "    # Run detection using detect.py logic\n",
    "    from detect import detect_single_image\n",
    "    \n",
    "    # This would use the author's detect.py script\n",
    "    # For now, we'll use a simplified version\n",
    "    print(f\"\\nTo test on image '{image_path}', run:\")\n",
    "    print(f\"python detect.py -m {model_path or './weights/mobilenet0.25_Final.pth'} --image_path {image_path}\")\n",
    "\n",
    "print(\"Detection functions ready.\")\n",
    "print(\"Place test images in the notebook directory and use test_single_image('image.jpg')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to ONNX\n",
    "def export_onnx(model, save_path='./weights/featherface_baseline.onnx', input_size=640):\n",
    "    \"\"\"Export model to ONNX format\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(1, 3, input_size, input_size).to(device)\n",
    "    \n",
    "    print(f\"Exporting to {save_path}...\")\n",
    "    \n",
    "    # Export\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        save_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['bbox', 'confidence', 'landmarks'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'bbox': {0: 'batch_size'},\n",
    "            'confidence': {0: 'batch_size'},\n",
    "            'landmarks': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Model exported to {save_path}\")\n",
    "    print(f\"  Size: {os.path.getsize(save_path) / 1e6:.2f} MB\")\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    import onnx\n",
    "    onnx_model = onnx.load(save_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"✓ ONNX model is valid\")\n",
    "\n",
    "# Uncomment to export\n",
    "# export_onnx(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Results\n",
    "\n",
    "### Expected Baseline Results (from paper)\n",
    "- Model: FeatherFace with MobileNetV1 0.25x\n",
    "- Parameters: 0.49M\n",
    "- Performance on WIDERFace:\n",
    "  - Easy: 90.8%\n",
    "  - Medium: 88.1%\n",
    "  - Hard: 73.8%\n",
    "\n",
    "### Training Commands Summary\n",
    "```bash\n",
    "# Training\n",
    "CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 train.py --network mobile0.25\n",
    "\n",
    "# Testing\n",
    "python test_widerface.py --trained_model ./weights/mobilenet0.25_Final.pth --network mobile0.25 --origin_size True\n",
    "\n",
    "# Evaluation\n",
    "cd ./widerface_evaluate\n",
    "python evaluation.py -p ./widerface_txt -g ./eval_tools/ground_truth\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "1. Complete full training (250 epochs)\n",
    "2. Evaluate on WIDERFace test set\n",
    "3. Proceed to Phase 2: FeatherFace V2 optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save notebook execution summary\n",
    "summary = {\n",
    "    'notebook': '01_train_evaluate_featherface.ipynb',\n",
    "    'model': 'FeatherFace Baseline',\n",
    "    'backbone': 'MobileNetV1 0.25x',\n",
    "    'parameters': f\"{total_params/1e6:.2f}M\" if 'total_params' in locals() else \"0.49M\",\n",
    "    'dataset': 'WIDERFace',\n",
    "    'config': cfg_mnet['name'],\n",
    "    'expected_map': {'easy': 90.8, 'medium': 88.1, 'hard': 73.8},\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "import json\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "with open('./results/baseline_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "print(\"✅ Baseline notebook execution complete!\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
