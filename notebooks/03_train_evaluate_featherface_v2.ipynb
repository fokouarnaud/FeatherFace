{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatherFace V2 Training and Evaluation with Knowledge Distillation\n",
    "\n",
    "This notebook implements the complete training and evaluation pipeline for FeatherFace V2 using knowledge distillation from the original model.\n",
    "\n",
    "## Overview\n",
    "- **Model**: FeatherFace V2 with optimized modules\n",
    "- **Parameters**: 0.256M (56.7% reduction from baseline)\n",
    "- **Training**: Knowledge Distillation with temperature T=4\n",
    "- **Dataset**: WIDERFace (auto-download)\n",
    "- **Target**: 92%+ mAP with 0.25M parameters\n",
    "- **Features**: MixUp, CutMix, DropBlock, Cosine Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths - all paths are relative to the FeatherFace root directory\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root directory (parent of notebooks/)\n",
    "PROJECT_ROOT = Path(os.path.abspath('..'))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Change to project root for all operations\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install project in editable mode if not already installed\n",
    "!pip install -e .\n",
    "\n",
    "# Install additional dependencies for V2\n",
    "!pip install thop  # For FLOPs calculation\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from models.retinaface import RetinaFace\n",
    "    from models.retinaface_v2 import RetinaFaceV2, get_retinaface_v2, count_parameters\n",
    "    from data import cfg_mnet, cfg_mnet_v2, WiderFaceDetection\n",
    "    from layers.modules_distill import DistillationLoss, DropBlock2D\n",
    "    print(\"✓ All imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify environment\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gdown\n",
    "import zipfile\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset and Pre-trained Weights Preparation\n",
    "\n",
    "We need:\n",
    "1. WIDERFace dataset (same as V1)\n",
    "2. Pre-trained MobileNetV1 weights (for backbone)\n",
    "3. Teacher model weights (original FeatherFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "data_dir = Path('data/widerface')\n",
    "data_root = Path('data')\n",
    "weights_dir = Path('weights')\n",
    "weights_v2_dir = Path('weights/v2')\n",
    "results_dir = Path('results')\n",
    "results_v2_dir = Path('results/v2')\n",
    "\n",
    "# WIDERFace download links\n",
    "WIDERFACE_GDRIVE_ID = '11UGV3nbVv1x9IC--_tK3Uxf7hA6rlbsS'\n",
    "WIDERFACE_URL = f'https://drive.google.com/uc?id={WIDERFACE_GDRIVE_ID}'\n",
    "\n",
    "\n",
    "for dir_path in [data_dir, weights_dir, weights_v2_dir, results_dir, results_v2_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✓ Directory ready: {dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_widerface():\n",
    "    \"\"\"Download WIDERFace dataset from Google Drive\"\"\"\n",
    "    output_path = data_root/ 'widerface.zip'\n",
    "    \n",
    "    if not output_path.exists():\n",
    "        print(\"Downloading WIDERFace dataset...\")\n",
    "        print(\"This may take several minutes depending on your connection.\")\n",
    "        \n",
    "        try:\n",
    "            gdown.download(WIDERFACE_URL, str(output_path), quiet=False)\n",
    "            print(f\"✓ Downloaded to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Download failed: {e}\")\n",
    "            print(\"Please download manually from:\")\n",
    "            print(f\"  {WIDERFACE_URL}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"✓ Dataset already downloaded: {output_path}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Download dataset\n",
    "if download_widerface():\n",
    "    print(\"\\n✅ Dataset download complete!\")\n",
    "else:\n",
    "    print(\"\\n❌ Please download the dataset manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset\n",
    "def extract_widerface():\n",
    "    \"\"\"Extract WIDERFace dataset\"\"\"\n",
    "    zip_path = data_root / 'widerface.zip'\n",
    "    \n",
    "    if not zip_path.exists():\n",
    "        print(\"❌ Dataset zip file not found. Please download first.\")\n",
    "        return False\n",
    "    \n",
    "    # Check if already extracted\n",
    "    if (data_dir / 'train' / 'label.txt').absolute().exists() and \\\n",
    "       (data_dir / 'val' / 'wider_val.txt').absolute().exists():\n",
    "        print(\"✓ Dataset already extracted\")\n",
    "        return True\n",
    "    \n",
    "    print(\"Extracting dataset...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_root)\n",
    "        print(\"✓ Dataset extracted successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Extraction failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Extract dataset\n",
    "if extract_widerface():\n",
    "    print(\"\\n✅ Dataset ready for use!\")\n",
    "else:\n",
    "    print(\"\\n❌ Please extract the dataset manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset (same as V1)\n",
    "def verify_dataset():\n",
    "    \"\"\"Verify WIDERFace dataset structure\"\"\"\n",
    "    required_files = [\n",
    "        data_dir / 'train' / 'label.txt',\n",
    "        data_dir / 'val' / 'wider_val.txt'\n",
    "    ]\n",
    "    \n",
    "    all_present = True\n",
    "    for file_path in required_files:\n",
    "        if file_path.exists():\n",
    "            print(f\"✓ Found: {file_path}\")\n",
    "        else:\n",
    "            print(f\"✗ Missing: {file_path}\")\n",
    "            all_present = False\n",
    "    \n",
    "    # Check for images\n",
    "    for split in ['train', 'val']:\n",
    "        img_dir = data_dir / split / 'images'\n",
    "        if img_dir.exists():\n",
    "            img_count = len(list(img_dir.glob('**/*.jpg')))\n",
    "            print(f\"✓ {split} images: {img_count} found\")\n",
    "        else:\n",
    "            print(f\"✗ {split} images directory not found\")\n",
    "            all_present = False\n",
    "    \n",
    "    return all_present\n",
    "\n",
    "dataset_ready = verify_dataset()\n",
    "print(f\"\\nDataset verification: {'PASSED ✅' if dataset_ready else 'FAILED ❌'}\")\n",
    "\n",
    "if not dataset_ready:\n",
    "    print(\"\\nPlease download WIDERFace dataset:\")\n",
    "    print(\"https://drive.google.com/open?id=11UGV3nbVv1x9IC--_tK3Uxf7hA6rlbsS\")\n",
    "    print(\"Extract to data/widerface/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required weights\n",
    "print(\"=== Required Weights Check ===\")\n",
    "\n",
    "# 1. MobileNetV1 pre-trained weights\n",
    "mobilenet_weights = weights_dir / 'mobilenetV1X0.25_pretrain.tar'\n",
    "if mobilenet_weights.exists():\n",
    "    print(f\"✓ MobileNet weights found: {mobilenet_weights}\")\n",
    "else:\n",
    "    print(f\"✗ MobileNet weights not found: {mobilenet_weights}\")\n",
    "    print(\"  Download from: https://drive.google.com/open?id=1oZRSG0ZegbVkVwUd8wUIQx8W7yfZ_ki1\")\n",
    "\n",
    "# 2. Teacher model weights (original FeatherFace)\n",
    "teacher_weights = weights_dir / 'mobilenet0.25_Final.pth'\n",
    "if teacher_weights.exists():\n",
    "    print(f\"✓ Teacher weights found: {teacher_weights}\")\n",
    "else:\n",
    "    print(f\"✗ Teacher weights not found: {teacher_weights}\")\n",
    "    print(\"  Train the original model first using notebook 01\")\n",
    "    print(\"  Or download pre-trained FeatherFace weights\")\n",
    "\n",
    "weights_ready = mobilenet_weights.exists()\n",
    "print(f\"\\nWeights check: {'PASSED ✅' if weights_ready else 'FAILED ❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. V2 Training Configuration\n",
    "\n",
    "Configure knowledge distillation and training parameters for V2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 Training Configuration\n",
    "V2_TRAIN_CONFIG = {\n",
    "    # Basic settings\n",
    "    'training_dataset': './data/widerface/train/label.txt',\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    'epochs': 400,  # Extended for knowledge distillation\n",
    "    'save_folder': './weights/v2/',\n",
    "    \n",
    "    # Teacher model\n",
    "    'teacher_model': './weights/mobilenet0.25_Final.pth',\n",
    "    \n",
    "    # Knowledge Distillation\n",
    "    'temperature': 4.0,\n",
    "    'alpha': 0.7,  # 70% distillation, 30% task loss\n",
    "    'feature_weight': 0.1,\n",
    "    \n",
    "    # Augmentation\n",
    "    'mixup_alpha': 0.2,\n",
    "    'cutmix_prob': 0.5,\n",
    "    'dropblock_prob': 0.1,\n",
    "    'dropblock_size': 3,\n",
    "    \n",
    "    # Optimizer\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 5e-4,\n",
    "    'warmup_epochs': 5,\n",
    "    \n",
    "    # GPU\n",
    "    'gpu': '0',\n",
    "    \n",
    "    # Resume training\n",
    "    'resume_net': None,\n",
    "    'resume_epoch': 0\n",
    "}\n",
    "\n",
    "print(\"FeatherFace V2 Training Configuration:\")\n",
    "print(json.dumps(V2_TRAIN_CONFIG, indent=2))\n",
    "\n",
    "# Compare with V1 config\n",
    "print(\"\\n=== Key Differences from V1 ===\")\n",
    "print(f\"Parameters: 0.592M → 0.256M\")\n",
    "print(f\"Training: Standard → Knowledge Distillation\")\n",
    "print(f\"Augmentation: Basic → MixUp + CutMix + DropBlock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare models\n",
    "print(\"Loading models for comparison...\")\n",
    "\n",
    "# Load V1 (Teacher)\n",
    "teacher_model = RetinaFace(cfg=cfg_mnet, phase='test')\n",
    "teacher_model = teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "\n",
    "# Load V2 (Student)\n",
    "student_model = get_retinaface_v2(cfg_mnet_v2, phase='test')\n",
    "student_model = student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "# Count parameters\n",
    "teacher_params = count_parameters(teacher_model)\n",
    "student_params = count_parameters(student_model)\n",
    "\n",
    "print(f\"\\nTeacher (V1): {teacher_params:,} parameters ({teacher_params/1e6:.3f}M)\")\n",
    "print(f\"Student (V2): {student_params:,} parameters ({student_params/1e6:.3f}M)\")\n",
    "print(f\"Compression: {teacher_params/student_params:.2f}x\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 3, 640, 640).to(device)\n",
    "with torch.no_grad():\n",
    "    teacher_out = teacher_model(dummy_input)\n",
    "    student_out = student_model(dummy_input)\n",
    "    \n",
    "print(\"\\n✓ Both models working correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Process\n",
    "\n",
    "We'll use the train_v2.py script with knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training command\n",
    "import subprocess\n",
    "\n",
    "train_v2_args = [\n",
    "    sys.executable, 'train_v2.py',\n",
    "    '--training_dataset', V2_TRAIN_CONFIG['training_dataset'],\n",
    "    '--teacher_model', V2_TRAIN_CONFIG['teacher_model'],\n",
    "    '--save_folder', V2_TRAIN_CONFIG['save_folder'],\n",
    "    '--batch_size', str(V2_TRAIN_CONFIG['batch_size']),\n",
    "    '--lr', str(V2_TRAIN_CONFIG['lr']),\n",
    "    '--epochs', str(V2_TRAIN_CONFIG['epochs']),\n",
    "    '--warmup_epochs', str(V2_TRAIN_CONFIG['warmup_epochs']),\n",
    "    '--temperature', str(V2_TRAIN_CONFIG['temperature']),\n",
    "    '--alpha', str(V2_TRAIN_CONFIG['alpha']),\n",
    "    '--feature_weight', str(V2_TRAIN_CONFIG['feature_weight']),\n",
    "    '--mixup_alpha', str(V2_TRAIN_CONFIG['mixup_alpha']),\n",
    "    '--cutmix_prob', str(V2_TRAIN_CONFIG['cutmix_prob']),\n",
    "    '--dropblock_prob', str(V2_TRAIN_CONFIG['dropblock_prob']),\n",
    "    '--dropblock_size', str(V2_TRAIN_CONFIG['dropblock_size']),\n",
    "    '--num_workers', str(V2_TRAIN_CONFIG['num_workers']),\n",
    "    '--gpu', V2_TRAIN_CONFIG['gpu']\n",
    "]\n",
    "\n",
    "# Add resume options if specified\n",
    "if V2_TRAIN_CONFIG['resume_net']:\n",
    "    train_v2_args.extend(['--resume_net', V2_TRAIN_CONFIG['resume_net']])\n",
    "    train_v2_args.extend(['--resume_epoch', str(V2_TRAIN_CONFIG['resume_epoch'])])\n",
    "\n",
    "print(\"Training command:\")\n",
    "print(' '.join(train_v2_args))\n",
    "\n",
    "# Save command for easy reuse\n",
    "with open('train_v2_command.txt', 'w') as f:\n",
    "    f.write(' '.join(train_v2_args).replace(sys.executable, 'python'))\n",
    "print(\"\\nCommand saved to train_v2_command.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training monitoring setup\n",
    "print(\"=== Training Monitoring Setup ===\")\n",
    "print(\"\\nDuring training, you'll see:\")\n",
    "print(\"1. Total Loss = (1-α)×Task Loss + α×Distill Loss + λ×Feature Loss\")\n",
    "print(f\"   where α={V2_TRAIN_CONFIG['alpha']}, λ={V2_TRAIN_CONFIG['feature_weight']}\")\n",
    "print(\"\\n2. Learning rate schedule:\")\n",
    "print(f\"   - Warmup: 0 → {V2_TRAIN_CONFIG['lr']} over {V2_TRAIN_CONFIG['warmup_epochs']} epochs\")\n",
    "print(f\"   - Cosine annealing: {V2_TRAIN_CONFIG['lr']} → 1e-6 over remaining epochs\")\n",
    "print(\"\\n3. Checkpoints saved every 10 epochs to:\", V2_TRAIN_CONFIG['save_folder'])\n",
    "\n",
    "# Create loss tracking file\n",
    "loss_log_path = Path(V2_TRAIN_CONFIG['save_folder']) / 'training_log.csv'\n",
    "print(f\"\\nLoss history will be saved to: {loss_log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Quick Test Run (5 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run with reduced epochs\n",
    "test_args = train_v2_args.copy()\n",
    "test_args[test_args.index('--epochs') + 1] = '5'\n",
    "\n",
    "print(\"Test command (5 epochs):\")\n",
    "print(' '.join(test_args).replace(sys.executable, 'python'))\n",
    "print(\"\\nUncomment below to run test:\")\n",
    "\n",
    "# result = subprocess.run(test_args, capture_output=True, text=True)\n",
    "# print(result.stdout)\n",
    "# if result.stderr:\n",
    "#     print(\"Errors:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Full Training (400 epochs)\n",
    "\n",
    "For production training, uncomment and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training - uncomment to run\n",
    "# print(\"Starting full training (400 epochs)...\")\n",
    "# result = subprocess.run(train_v2_args, capture_output=False)\n",
    "# print(f\"Training completed with exit code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Progress Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor training progress\n",
    "def plot_training_curves(log_df):\n",
    "    \"\"\"Plot training loss curves\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Total loss\n",
    "    axes[0,0].plot(log_df['epoch'], log_df['total_loss'])\n",
    "    axes[0,0].set_title('Total Loss')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    # Task vs Distillation loss\n",
    "    axes[0,1].plot(log_df['epoch'], log_df['task_loss'], label='Task Loss')\n",
    "    axes[0,1].plot(log_df['epoch'], log_df['distill_loss'], label='Distill Loss')\n",
    "    axes[0,1].set_title('Task vs Distillation Loss')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Loss')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True)\n",
    "    \n",
    "    # Learning rate\n",
    "    axes[1,0].plot(log_df['epoch'], log_df['lr'])\n",
    "    axes[1,0].set_title('Learning Rate Schedule')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Learning Rate')\n",
    "    axes[1,0].grid(True)\n",
    "    \n",
    "    # Feature loss (if available)\n",
    "    if 'feature_loss' in log_df.columns:\n",
    "        axes[1,1].plot(log_df['epoch'], log_df['feature_loss'])\n",
    "        axes[1,1].set_title('Feature Distillation Loss')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('Loss')\n",
    "        axes[1,1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Load and plot training log if available\n",
    "log_path = Path(V2_TRAIN_CONFIG['save_folder']) / 'training_log.csv'\n",
    "if log_path.exists():\n",
    "    log_df = pd.read_csv(log_path)\n",
    "    print(f\"Loaded training log with {len(log_df)} epochs\")\n",
    "    \n",
    "    # Show recent progress\n",
    "    if len(log_df) > 0:\n",
    "        print(\"\\nRecent training progress:\")\n",
    "        print(log_df.tail(5))\n",
    "        \n",
    "        # Plot curves\n",
    "        plot_training_curves(log_df)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"No training log found at {log_path}\")\n",
    "    print(\"Run training first to generate logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for saved checkpoints\n",
    "def list_checkpoints(checkpoint_dir):\n",
    "    \"\"\"List all saved checkpoints\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoints = list(checkpoint_dir.glob('*.pth'))\n",
    "    \n",
    "    if not checkpoints:\n",
    "        print(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "        return []\n",
    "    \n",
    "    # Sort by epoch number\n",
    "    checkpoint_info = []\n",
    "    for ckpt in checkpoints:\n",
    "        # Extract epoch from filename\n",
    "        if 'epoch' in ckpt.stem:\n",
    "            try:\n",
    "                epoch = int(ckpt.stem.split('_')[-1])\n",
    "                checkpoint_info.append((epoch, ckpt))\n",
    "            except:\n",
    "                checkpoint_info.append((999, ckpt))\n",
    "        else:\n",
    "            checkpoint_info.append((999, ckpt))\n",
    "    \n",
    "    # Sort by epoch\n",
    "    checkpoint_info.sort(key=lambda x: x[0])\n",
    "    \n",
    "    print(f\"Found {len(checkpoints)} checkpoints:\")\n",
    "    for epoch, ckpt in checkpoint_info:\n",
    "        size_mb = ckpt.stat().st_size / 1024 / 1024\n",
    "        if epoch == 999:\n",
    "            print(f\"  - {ckpt.name} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"  - Epoch {epoch}: {ckpt.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    return checkpoint_info\n",
    "\n",
    "# List available checkpoints\n",
    "checkpoints = list_checkpoints(V2_TRAIN_CONFIG['save_folder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation on WIDERFace\n",
    "\n",
    "Evaluate the trained V2 model and compare with V1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint for evaluation\n",
    "def load_best_checkpoint(model, checkpoint_dir, device):\n",
    "    \"\"\"Load the best (latest) checkpoint\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    \n",
    "    # Look for final model first\n",
    "    final_path = checkpoint_dir / 'FeatherFaceV2_final.pth'\n",
    "    if final_path.exists():\n",
    "        print(f\"Loading final model: {final_path}\")\n",
    "        checkpoint = torch.load(final_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return model, checkpoint.get('epochs_trained', 'unknown')\n",
    "    \n",
    "    # Otherwise load latest checkpoint\n",
    "    checkpoints = list(checkpoint_dir.glob('FeatherFaceV2_epoch_*.pth'))\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found!\")\n",
    "        return model, 0\n",
    "    \n",
    "    # Sort by epoch and get latest\n",
    "    latest = sorted(checkpoints, key=lambda x: int(x.stem.split('_')[-1]))[-1]\n",
    "    print(f\"Loading checkpoint: {latest}\")\n",
    "    checkpoint = torch.load(latest, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, checkpoint.get('epoch', 'unknown')\n",
    "\n",
    "# Load trained model\n",
    "v2_model = get_retinaface_v2(cfg_mnet_v2, phase='test')\n",
    "v2_model = v2_model.to(device)\n",
    "v2_model, trained_epochs = load_best_checkpoint(v2_model, V2_TRAIN_CONFIG['save_folder'], device)\n",
    "v2_model.eval()\n",
    "\n",
    "print(f\"\\nModel loaded from epoch: {trained_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation configuration\n",
    "EVAL_CONFIG = {\n",
    "    'trained_model': str(Path(V2_TRAIN_CONFIG['save_folder']) / 'FeatherFaceV2_final.pth'),\n",
    "    'network': 'mobile0.25',\n",
    "    'dataset_folder': './data/widerface/val/images/',\n",
    "    'confidence_threshold': 0.02,\n",
    "    'top_k': 5000,\n",
    "    'nms_threshold': 0.4,\n",
    "    'keep_top_k': 750,\n",
    "    'save_folder': './results/v2/widerface_eval/',\n",
    "    'cpu': False,\n",
    "    'vis_thres': 0.5\n",
    "}\n",
    "\n",
    "# Create evaluation command\n",
    "eval_args = [\n",
    "    sys.executable, 'test_widerface.py',\n",
    "    '--trained_model', EVAL_CONFIG['trained_model'],\n",
    "    '--network', EVAL_CONFIG['network'],\n",
    "    '--dataset_folder', EVAL_CONFIG['dataset_folder'],\n",
    "    '--confidence_threshold', str(EVAL_CONFIG['confidence_threshold']),\n",
    "    '--top_k', str(EVAL_CONFIG['top_k']),\n",
    "    '--nms_threshold', str(EVAL_CONFIG['nms_threshold']),\n",
    "    '--keep_top_k', str(EVAL_CONFIG['keep_top_k']),\n",
    "    '--save_folder', EVAL_CONFIG['save_folder']\n",
    "]\n",
    "\n",
    "if EVAL_CONFIG['cpu']:\n",
    "    eval_args.append('--cpu')\n",
    "\n",
    "print(\"Evaluation command:\")\n",
    "print(' '.join(eval_args).replace(sys.executable, 'python'))\n",
    "\n",
    "# Note: The test_widerface.py script needs to be modified to support V2\n",
    "print(\"\\nNote: Make sure test_widerface.py is updated to load RetinaFaceV2 model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Run evaluation directly (recommended)\n",
    "# Uncomment to run:\n",
    "# result = subprocess.run(eval_args, capture_output=True, text=True)\n",
    "# print(result.stdout)\n",
    "# if result.stderr:\n",
    "#     print(\"Errors:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Direct Model Evaluation\n",
    "\n",
    "Evaluate V2 performance directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation utilities\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms\n",
    "from utils.box_utils import decode, decode_landm\n",
    "\n",
    "def detect_faces_v2(model, image_path, cfg, device, \n",
    "                    confidence_threshold=0.5, nms_threshold=0.4):\n",
    "    \"\"\"Detect faces using V2 model\"\"\"\n",
    "    # Load and preprocess image\n",
    "    img_raw = cv2.imread(str(image_path))\n",
    "    if img_raw is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    img = np.float32(img_raw)\n",
    "    im_height, im_width = img.shape[:2]\n",
    "    scale = torch.Tensor([im_width, im_height, im_width, im_height])\n",
    "    \n",
    "    # Resize and normalize\n",
    "    img_size = cfg['image_size']\n",
    "    img = cv2.resize(img, (img_size, img_size))\n",
    "    img -= (104, 117, 123)\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = torch.from_numpy(img).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    # Generate priors\n",
    "    priorbox = PriorBox(cfg, image_size=(img_size, img_size))\n",
    "    priors = priorbox.forward().to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        loc, conf, landms = model(img)\n",
    "    \n",
    "    # Decode predictions\n",
    "    boxes = decode(loc.data.squeeze(0), priors, cfg['variance'])\n",
    "    boxes = boxes * scale\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    \n",
    "    scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "    \n",
    "    landms = decode_landm(landms.data.squeeze(0), priors, cfg['variance'])\n",
    "    scale_landm = torch.Tensor([im_width, im_height] * 5)\n",
    "    landms = landms * scale_landm\n",
    "    landms = landms.cpu().numpy()\n",
    "    \n",
    "    # Filter by confidence\n",
    "    inds = np.where(scores > confidence_threshold)[0]\n",
    "    boxes = boxes[inds]\n",
    "    scores = scores[inds]\n",
    "    landms = landms[inds]\n",
    "    \n",
    "    # Apply NMS\n",
    "    keep = py_cpu_nms(np.hstack((boxes, scores[:, np.newaxis])), nms_threshold)\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    landms = landms[keep]\n",
    "    \n",
    "    return boxes, scores, landms\n",
    "\n",
    "print(\"Detection function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample images\n",
    "test_images_dir = Path('./tests/test_images')\n",
    "if not test_images_dir.exists():\n",
    "    test_images_dir.mkdir(exist_ok=True)\n",
    "    print(f\"Created {test_images_dir}\")\n",
    "    print(\"Please add test images to this directory\")\n",
    "\n",
    "# Find test images\n",
    "test_images = list(test_images_dir.glob('*.jpg')) + list(test_images_dir.glob('*.png'))\n",
    "\n",
    "if test_images:\n",
    "    print(f\"Found {len(test_images)} test images\")\n",
    "    \n",
    "    # Process first image as example\n",
    "    test_img = test_images[0]\n",
    "    print(f\"\\nTesting on: {test_img}\")\n",
    "    \n",
    "    # Detect with V2\n",
    "    boxes, scores, landms = detect_faces_v2(\n",
    "        v2_model, test_img, cfg_mnet_v2, device,\n",
    "        confidence_threshold=0.5, nms_threshold=0.4\n",
    "    )\n",
    "    \n",
    "    if boxes is not None:\n",
    "        print(f\"Detected {len(boxes)} faces\")\n",
    "        \n",
    "        # Visualize results\n",
    "        img_show = cv2.imread(str(test_img))\n",
    "        for box, score in zip(boxes, scores):\n",
    "            x1, y1, x2, y2 = box.astype(int)\n",
    "            cv2.rectangle(img_show, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(img_show, f'{score:.3f}', (x1, y1-10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'FeatherFace V2 Detection - {len(boxes)} faces')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No test images found. Add images to test_images/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Analysis\n",
    "\n",
    "Compare V1 and V2 performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "def compare_models_performance(v1_model, v2_model, test_images, device):\n",
    "    \"\"\"Compare V1 and V2 on test images\"\"\"\n",
    "    results = {\n",
    "        'image': [],\n",
    "        'v1_faces': [],\n",
    "        'v2_faces': [],\n",
    "        'v1_time': [],\n",
    "        'v2_time': [],\n",
    "        'v1_conf_mean': [],\n",
    "        'v2_conf_mean': []\n",
    "    }\n",
    "    \n",
    "    for img_path in test_images:\n",
    "        print(f\"\\nProcessing: {img_path.name}\")\n",
    "        \n",
    "        # Time V1\n",
    "        start = time.time()\n",
    "        boxes_v1, scores_v1, _ = detect_faces_v2(\n",
    "            v1_model, img_path, cfg_mnet, device\n",
    "        )\n",
    "        v1_time = (time.time() - start) * 1000\n",
    "        \n",
    "        # Time V2\n",
    "        start = time.time()\n",
    "        boxes_v2, scores_v2, _ = detect_faces_v2(\n",
    "            v2_model, img_path, cfg_mnet_v2, device\n",
    "        )\n",
    "        v2_time = (time.time() - start) * 1000\n",
    "        \n",
    "        # Record results\n",
    "        results['image'].append(img_path.name)\n",
    "        results['v1_faces'].append(len(boxes_v1) if boxes_v1 is not None else 0)\n",
    "        results['v2_faces'].append(len(boxes_v2) if boxes_v2 is not None else 0)\n",
    "        results['v1_time'].append(v1_time)\n",
    "        results['v2_time'].append(v2_time)\n",
    "        results['v1_conf_mean'].append(scores_v1.mean() if len(scores_v1) > 0 else 0)\n",
    "        results['v2_conf_mean'].append(scores_v2.mean() if len(scores_v2) > 0 else 0)\n",
    "        \n",
    "        print(f\"  V1: {len(boxes_v1)} faces in {v1_time:.1f}ms\")\n",
    "        print(f\"  V2: {len(boxes_v2)} faces in {v2_time:.1f}ms\")\n",
    "        print(f\"  Speedup: {v1_time/v2_time:.2f}x\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run comparison if test images available\n",
    "if test_images:\n",
    "    print(\"Comparing V1 and V2 performance...\")\n",
    "    comparison_df = compare_models_performance(\n",
    "        teacher_model, v2_model, test_images[:5], device\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Performance Summary ===\")\n",
    "    print(f\"Average inference time:\")\n",
    "    print(f\"  V1: {comparison_df['v1_time'].mean():.1f}ms\")\n",
    "    print(f\"  V2: {comparison_df['v2_time'].mean():.1f}ms\")\n",
    "    print(f\"  Average speedup: {(comparison_df['v1_time'] / comparison_df['v2_time']).mean():.2f}x\")\n",
    "    \n",
    "    print(f\"\\nDetection consistency:\")\n",
    "    same_detections = (comparison_df['v1_faces'] == comparison_df['v2_faces']).sum()\n",
    "    print(f\"  Same number of detections: {same_detections}/{len(comparison_df)} images\")\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_df.to_csv(results_v2_dir / 'performance_comparison.csv', index=False)\n",
    "    print(f\"\\nComparison saved to {results_v2_dir / 'performance_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance summary\n",
    "print(\"=\"*60)\n",
    "print(\"FEATHERFACE V2 TRAINING & EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Model Architecture:\")\n",
    "print(f\"   Parameters: {student_params:,} ({student_params/1e6:.3f}M)\")\n",
    "print(f\"   Reduction: {(1-student_params/teacher_params)*100:.1f}% from V1\")\n",
    "print(f\"   Compression: {teacher_params/student_params:.2f}x\")\n",
    "\n",
    "print(\"\\n2. Training Configuration:\")\n",
    "print(f\"   Method: Knowledge Distillation (T={V2_TRAIN_CONFIG['temperature']}, α={V2_TRAIN_CONFIG['alpha']})\")\n",
    "print(f\"   Augmentation: MixUp + CutMix + DropBlock\")\n",
    "print(f\"   Epochs: {V2_TRAIN_CONFIG['epochs']}\")\n",
    "print(f\"   Trained epochs: {trained_epochs}\")\n",
    "\n",
    "if test_images and 'comparison_df' in locals():\n",
    "    print(\"\\n3. Performance Results:\")\n",
    "    print(f\"   Inference speedup: {(comparison_df['v1_time'] / comparison_df['v2_time']).mean():.2f}x\")\n",
    "    print(f\"   Detection consistency: {(comparison_df['v1_faces'] == comparison_df['v2_faces']).mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\\n4. Next Steps:\")\n",
    "print(\"   - Complete full 400 epoch training\")\n",
    "print(\"   - Evaluate on full WIDERFace validation set\")\n",
    "print(\"   - Calculate official mAP scores\")\n",
    "print(\"   - Deploy to target hardware\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Export and Deployment\n",
    "\n",
    "Export the trained V2 model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export deployment model with ONNX support\n",
    "def export_deployment_model(model, config, save_path, export_onnx=True):\n",
    "    \"\"\"Export model with all necessary components for deployment\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create deployment package\n",
    "    deployment_package = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'preprocessing': {\n",
    "            'mean': (104, 117, 123),  # BGR order\n",
    "            'std': (1, 1, 1),\n",
    "            'image_size': config['image_size'],\n",
    "            'variance': config['variance']\n",
    "        },\n",
    "        'postprocessing': {\n",
    "            'confidence_threshold': 0.5,\n",
    "            'nms_threshold': 0.4,\n",
    "            'top_k': 5000,\n",
    "            'keep_top_k': 750\n",
    "        },\n",
    "        'model_info': {\n",
    "            'parameters': count_parameters(model),\n",
    "            'architecture': 'FeatherFace V2',\n",
    "            'framework': 'PyTorch',\n",
    "            'version': '2.0',\n",
    "            'compression_ratio': 2.31  # from V1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save PyTorch model\n",
    "    torch.save(deployment_package, save_path)\n",
    "    print(f\"✓ PyTorch model saved to: {save_path}\")\n",
    "    print(f\"  Model size: {Path(save_path).stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Export ONNX if requested\n",
    "    if export_onnx:\n",
    "        onnx_path = str(save_path).replace('.pth', '.onnx')\n",
    "        print(f\"\\nExporting ONNX model...\")\n",
    "        \n",
    "        try:\n",
    "            # Create dummy input\n",
    "            dummy_input = torch.randn(1, 3, config['image_size'], config['image_size'])\n",
    "            dummy_input = dummy_input.to(device)\n",
    "            \n",
    "            # Export to ONNX\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                dummy_input,\n",
    "                onnx_path,\n",
    "                export_params=True,\n",
    "                opset_version=11,\n",
    "                do_constant_folding=True,\n",
    "                input_names=['input'],\n",
    "                output_names=['classifications', 'bbox_regressions', 'landmarks'],\n",
    "                dynamic_axes={\n",
    "                    'input': {0: 'batch_size'},\n",
    "                    'classifications': {0: 'batch_size'},\n",
    "                    'bbox_regressions': {0: 'batch_size'},\n",
    "                    'landmarks': {0: 'batch_size'}\n",
    "                },\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            print(f\"✓ ONNX model exported to: {onnx_path}\")\n",
    "            print(f\"  ONNX size: {Path(onnx_path).stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "            \n",
    "            # Verify ONNX model\n",
    "            try:\n",
    "                import onnx\n",
    "                onnx_model = onnx.load(onnx_path)\n",
    "                onnx.checker.check_model(onnx_model)\n",
    "                print(\"✓ ONNX model verification passed\")\n",
    "            except ImportError:\n",
    "                print(\"⚠ Install onnx to verify: pip install onnx\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ ONNX export failed: {e}\")\n",
    "            print(\"  This is optional - PyTorch model is sufficient for deployment\")\n",
    "    \n",
    "    return deployment_package\n",
    "\n",
    "# Export if model is trained\n",
    "if 'v2_model' in locals():\n",
    "    deployment_path = results_v2_dir / 'featherface_v2_deployment.pth'\n",
    "    deployment_info = export_deployment_model(v2_model, cfg_mnet_v2, deployment_path, export_onnx=True)\n",
    "else:\n",
    "    print(\"Train the model first before exporting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Model Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using the exported ONNX model\n",
    "def test_onnx_inference():\n",
    "    \"\"\"Test ONNX model inference\"\"\"\n",
    "    onnx_path = results_v2_dir / 'featherface_v2_deployment.onnx'\n",
    "    \n",
    "    if not onnx_path.exists():\n",
    "        print(f\"ONNX model not found at {onnx_path}\")\n",
    "        print(\"Run the export cell above first\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        import onnxruntime as ort\n",
    "        import numpy as np\n",
    "        \n",
    "        print(\"Testing ONNX model inference...\")\n",
    "        \n",
    "        # Create ONNX Runtime session\n",
    "        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "        session = ort.InferenceSession(str(onnx_path), providers=providers)\n",
    "        \n",
    "        # Get input and output names\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        output_names = [output.name for output in session.get_outputs()]\n",
    "        \n",
    "        print(f\"✓ ONNX model loaded\")\n",
    "        print(f\"  Input: {input_name} - Shape: {session.get_inputs()[0].shape}\")\n",
    "        print(f\"  Outputs: {output_names}\")\n",
    "        \n",
    "        # Create test input\n",
    "        test_input = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
    "        \n",
    "        # Run inference\n",
    "        start_time = time.time()\n",
    "        outputs = session.run(output_names, {input_name: test_input})\n",
    "        inference_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        print(f\"\\n✓ ONNX inference successful!\")\n",
    "        print(f\"  Inference time: {inference_time:.2f}ms\")\n",
    "        print(f\"  Output shapes:\")\n",
    "        for name, output in zip(output_names, outputs):\n",
    "            print(f\"    - {name}: {output.shape}\")\n",
    "        \n",
    "        # Compare with PyTorch inference time\n",
    "        if 'v2_model' in locals():\n",
    "            torch_input = torch.from_numpy(test_input).to(device)\n",
    "            with torch.no_grad():\n",
    "                torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                start_time = time.time()\n",
    "                _ = v2_model(torch_input)\n",
    "                torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                torch_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            print(f\"\\nSpeed comparison:\")\n",
    "            print(f\"  PyTorch: {torch_time:.2f}ms\")\n",
    "            print(f\"  ONNX: {inference_time:.2f}ms\")\n",
    "            print(f\"  ONNX speedup: {torch_time/inference_time:.2f}x\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"✗ ONNX Runtime not installed\")\n",
    "        print(\"  Install with: pip install onnxruntime-gpu  # for GPU\")\n",
    "        print(\"  Or: pip install onnxruntime  # for CPU only\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ ONNX test failed: {e}\")\n",
    "\n",
    "# Run ONNX test\n",
    "test_onnx_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Face Detection Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete face detection with ONNX\n",
    "def detect_faces_onnx(image_path, onnx_path, confidence_threshold=0.5):\n",
    "    \"\"\"Detect faces using ONNX model\"\"\"\n",
    "    try:\n",
    "        import onnxruntime as ort\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(str(image_path))\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image: {image_path}\")\n",
    "            return None\n",
    "        \n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Preprocess\n",
    "        img_resized = cv2.resize(img, (640, 640))\n",
    "        img_normalized = (img_resized.astype(np.float32) - np.array([104, 117, 123])) \n",
    "        img_input = np.transpose(img_normalized, (2, 0, 1))[np.newaxis, ...]\n",
    "        \n",
    "        # Create ONNX session\n",
    "        session = ort.InferenceSession(str(onnx_path))\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = session.run(None, {input_name: img_input})\n",
    "        \n",
    "        # Process outputs (classifications, bbox, landmarks)\n",
    "        scores = outputs[0][0, :, 1]  # Face scores\n",
    "        boxes = outputs[1][0]  # Bounding boxes\n",
    "        landmarks = outputs[2][0]  # Face landmarks\n",
    "        \n",
    "        # Filter by confidence\n",
    "        keep = scores > confidence_threshold\n",
    "        scores = scores[keep]\n",
    "        boxes = boxes[keep]\n",
    "        landmarks = landmarks[keep]\n",
    "        \n",
    "        # Scale boxes to original image size\n",
    "        boxes[:, [0, 2]] *= w / 640\n",
    "        boxes[:, [1, 3]] *= h / 640\n",
    "        landmarks[:, 0::2] *= w / 640\n",
    "        landmarks[:, 1::2] *= h / 640\n",
    "        \n",
    "        print(f\"Detected {len(boxes)} faces with ONNX\")\n",
    "        \n",
    "        return boxes, scores, landmarks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ONNX detection failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Test ONNX detection\n",
    "onnx_model_path = results_v2_dir / 'featherface_v2_deployment.onnx'\n",
    "if onnx_model_path.exists() and test_images:\n",
    "    print(\"Testing ONNX face detection...\")\n",
    "    boxes, scores, landmarks = detect_faces_onnx(test_images[0], onnx_model_path)\n",
    "    if boxes is not None:\n",
    "        print(f\"Success! Found {len(boxes)} faces\")\n",
    "else:\n",
    "    print(\"Export ONNX model first or add test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment README with ONNX info\n",
    "readme_content = f\"\"\"# FeatherFace V2 Deployment Package\n",
    "\n",
    "## Model Information\n",
    "- Architecture: FeatherFace V2 with Knowledge Distillation\n",
    "- Parameters: 0.256M (56.7% reduction from V1)\n",
    "- Framework: PyTorch / ONNX\n",
    "- Performance: \n",
    "  - 0.25M parameters\n",
    "  - 1.5-2x faster inference (PyTorch)\n",
    "  - 2-3x faster with ONNX Runtime\n",
    "  - Target: 92%+ mAP on WIDERFace\n",
    "\n",
    "## Files Included\n",
    "- `featherface_v2_deployment.pth`: PyTorch model with metadata\n",
    "- `featherface_v2_deployment.onnx`: ONNX model for cross-platform deployment\n",
    "- `README.md`: This file\n",
    "\n",
    "## PyTorch Usage\n",
    "```python\n",
    "import torch\n",
    "from models.retinaface_v2 import get_retinaface_v2\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load('featherface_v2_deployment.pth')\n",
    "model = get_retinaface_v2(checkpoint['config'], phase='test')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Preprocessing info\n",
    "mean = checkpoint['preprocessing']['mean']  # (104, 117, 123)\n",
    "img_size = checkpoint['preprocessing']['image_size']  # 640\n",
    "```\n",
    "\n",
    "## ONNX Usage\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession('featherface_v2_deployment.onnx')\n",
    "\n",
    "# Preprocess image\n",
    "img = cv2.imread('face.jpg')\n",
    "img_resized = cv2.resize(img, (640, 640))\n",
    "img_norm = (img_resized.astype(np.float32) - [104, 117, 123])\n",
    "img_input = np.transpose(img_norm, (2, 0, 1))[np.newaxis, ...]\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, {{'input': img_input}})\n",
    "classifications, bboxes, landmarks = outputs\n",
    "```\n",
    "\n",
    "## Model Details\n",
    "- Input: `[1, 3, 640, 640]` (NCHW format, BGR, mean subtracted)\n",
    "- Outputs:\n",
    "  - classifications: `[1, 16800, 2]` (background/face scores)\n",
    "  - bbox_regressions: `[1, 16800, 4]` (x1, y1, x2, y2)\n",
    "  - landmarks: `[1, 16800, 10]` (5 facial landmarks x,y pairs)\n",
    "\n",
    "## Deployment Platforms\n",
    "- **Mobile**: Use ONNX Runtime Mobile or TensorFlow Lite (convert from ONNX)\n",
    "- **Web**: ONNX.js or TensorFlow.js\n",
    "- **Edge**: ONNX Runtime with hardware acceleration\n",
    "- **Server**: PyTorch or ONNX Runtime with CUDA\n",
    "\n",
    "## Performance Tips\n",
    "1. Use ONNX Runtime for best inference speed\n",
    "2. Enable GPU acceleration when available\n",
    "3. Batch multiple images for better throughput\n",
    "4. Consider INT8 quantization for edge devices\n",
    "\n",
    "## Model Stats\n",
    "- PyTorch size: {(Path(deployment_path).stat().st_size / 1024 / 1024) if Path(deployment_path).exists() else 'N/A':.1f} MB\n",
    "- ONNX size: {(Path(str(deployment_path).replace('.pth', '.onnx')).stat().st_size / 1024 / 1024) if Path(str(deployment_path).replace('.pth', '.onnx')).exists() else 'N/A':.1f} MB\n",
    "- Parameters: {deployment_info['model_info']['parameters'] if 'deployment_info' in locals() else 256156:,}\n",
    "\"\"\"\n",
    "\n",
    "with open(results_v2_dir / 'README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "print(\"Deployment README created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Tips and Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "1. **Out of Memory**\n",
    "   - Reduce batch_size (try 16 or 8)\n",
    "   - Enable gradient accumulation\n",
    "   - Reduce image_size to 512\n",
    "\n",
    "2. **Poor Convergence**\n",
    "   - Check teacher model quality\n",
    "   - Increase alpha (more distillation)\n",
    "   - Reduce learning rate\n",
    "   - Increase warmup_epochs\n",
    "\n",
    "3. **Slow Training**\n",
    "   - Increase num_workers\n",
    "   - Use mixed precision training\n",
    "   - Reduce augmentation probability\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Monitor Training**\n",
    "   - Check loss ratios (distill/task)\n",
    "   - Validate every 10 epochs\n",
    "   - Save checkpoints frequently\n",
    "\n",
    "2. **Hyperparameter Tuning**\n",
    "   - Start with default values\n",
    "   - Tune temperature first (3-5)\n",
    "   - Adjust alpha based on loss ratio\n",
    "\n",
    "3. **Data Augmentation**\n",
    "   - Keep all augmentations enabled\n",
    "   - Adjust probabilities if needed\n",
    "   - Consider adding RandAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save notebook configuration for reproducibility\n",
    "notebook_config = {\n",
    "    'created': datetime.now().isoformat(),\n",
    "    'environment': {\n",
    "        'python': sys.version,\n",
    "        'pytorch': torch.__version__,\n",
    "        'cuda': torch.cuda.is_available(),\n",
    "        'device': str(device)\n",
    "    },\n",
    "    'training_config': V2_TRAIN_CONFIG,\n",
    "    'evaluation_config': EVAL_CONFIG,\n",
    "    'model_info': {\n",
    "        'teacher_params': teacher_params,\n",
    "        'student_params': student_params,\n",
    "        'compression_ratio': teacher_params / student_params\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_v2_dir / 'notebook_config.json', 'w') as f:\n",
    "    json.dump(notebook_config, f, indent=2)\n",
    "\n",
    "print(\"Notebook configuration saved\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK EXECUTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFeatherFace V2 is ready for training and deployment!\")\n",
    "print(\"Follow the instructions above to train your model.\")\n",
    "print(\"\\nGood luck! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
