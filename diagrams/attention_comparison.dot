digraph AttentionComparison {
    // Attention Mechanisms Comparison: CBAM vs ODConv
    // Scientific comparison based on literature review 2025
    
    // Graph styling
    rankdir=TB;
    bgcolor="white";
    fontname="Arial";
    fontsize=14;
    
    // Node styling
    node [fontname="Arial", fontsize=11, shape=box, style=filled];
    edge [fontname="Arial", fontsize=9];
    
    // Title
    title [label="Attention Mechanisms Comparison\nSystematic Literature Review 2025", 
           fillcolor="#e1f5fe", fontsize=16, shape=ellipse];
    
    // CBAM Section
    subgraph cluster_cbam {
        label="üîµ CBAM Baseline (Woo et al. ECCV 2018)";
        color=blue;
        style=filled;
        fillcolor="#e3f2fd";
        
        cbam_overview [label="CBAM: Convolutional Block Attention Module\n2D Attention Mechanism", 
                       fillcolor="#bbdefb", fontsize=12, style=bold];
        
        subgraph cluster_cbam_components {
            label="CBAM Components";
            style=filled;
            fillcolor="#f8f9fa";
            
            cbam_channel [label="Channel Attention\n‚Ä¢ Global Average Pooling\n‚Ä¢ Global Max Pooling\n‚Ä¢ MLP (FC layers)\n‚Ä¢ Sigmoid activation\n\nFormula: Mc = œÉ(MLP(AvgPool(F)) + MLP(MaxPool(F)))", 
                         fillcolor="#e8eaf6"];
            
            cbam_spatial [label="Spatial Attention\n‚Ä¢ Channel-wise pooling\n‚Ä¢ 7√ó7 convolution\n‚Ä¢ Sigmoid activation\n\nFormula: Ms = œÉ(Conv7√ó7([AvgPool(F); MaxPool(F)]))", 
                         fillcolor="#e8eaf6"];
        }
        
        cbam_flow [label="CBAM Flow: F ‚Üí Channel Att ‚Üí Spatial Att ‚Üí F'\nF' = Ms(F) ‚äó Mc(F) ‚äó F", 
                   fillcolor="#c5cae9"];
        
        cbam_complexity [label="Complexity Analysis\n‚Ä¢ Parameters: O(C¬≤/r + 7√ó7√ó2)\n‚Ä¢ Computation: O(H√óW√óC¬≤/r)\n‚Ä¢ Local spatial modeling only\n‚Ä¢ Cannot capture long-range dependencies", 
                        fillcolor="#ffcdd2"];
    }
    
    // ODConv Section
    subgraph cluster_odconv {
        label="üü† ODConv Innovation (Li et al. ICLR 2022)";
        color=orange;
        style=filled;
        fillcolor="#fff3e0";
        
        odconv_overview [label="ODConv: Omni-Dimensional Dynamic Convolution\n4D Multidimensional Attention Mechanism", 
                        fillcolor="#ffcc02", fontsize=12, style=bold];
        
        subgraph cluster_odconv_components {
            label="ODConv 4D Components";
            style=filled;
            fillcolor="#f8f9fa";
            
            odconv_spatial [label="1. Spatial Attention (Œ±À¢)\n‚Ä¢ Location-wise importance\n‚Ä¢ Kernel spatial dimensions\n‚Ä¢ Œ±À¢ ‚àà ‚Ñù·¥¥·µèÀ£·µÇ·µè\n\nFormula: Œ±À¢ = œÉ(Conv1D(GAP(X)))", 
                           fillcolor="#ffe0b2"];
            
            odconv_input [label="2. Input Channel Attention (Œ±‚Å±)\n‚Ä¢ Input channel-wise importance\n‚Ä¢ Channel selection mechanism\n‚Ä¢ Œ±‚Å± ‚àà ‚Ñù·∂ú‚Å±\n\nFormula: Œ±‚Å± = œÉ(FC(GAP(X)))", 
                         fillcolor="#ffe0b2"];
            
            odconv_output [label="3. Output Channel Attention (Œ±·µí)\n‚Ä¢ Output channel-wise importance\n‚Ä¢ Feature map emphasis\n‚Ä¢ Œ±·µí ‚àà ‚Ñù·∂ú·µí\n\nFormula: Œ±·µí = œÉ(FC(GAP(X)))", 
                          fillcolor="#ffe0b2"];
            
            odconv_kernel [label="4. Kernel Attention (Œ±·µè)\n‚Ä¢ Kernel-wise importance\n‚Ä¢ Multiple kernel selection\n‚Ä¢ Œ±·µè ‚àà ‚Ñù·¥∑ (K=1 for efficiency)\n\nFormula: Œ±·µè = œÉ(FC(GAP(X)))", 
                          fillcolor="#ffe0b2"];
        }
        
        odconv_flow [label="ODConv Flow: X ‚Üí [Œ±À¢, Œ±‚Å±, Œ±·µí, Œ±·µè] ‚Üí WÃÉ ‚Üí Y\nWÃÉ = W ‚äô Œ±À¢ ‚äô Œ±‚Å± ‚äô Œ±·µí ‚äô Œ±·µè\nY = Conv(X, WÃÉ)", 
                    fillcolor="#ffb74d"];
        
        odconv_complexity [label="Complexity Analysis\n‚Ä¢ Parameters: O(C√óR√ó4) where R=reduction\n‚Ä¢ Computation: O(C√ók) vs O(C¬≤) for CBAM\n‚Ä¢ Multidimensional attention\n‚Ä¢ Superior long-range dependency modeling", 
                          fillcolor="#c8e6c9"];
    }
    
    // Performance Comparison
    subgraph cluster_performance {
        label="üìä Performance Comparison (Scientific Evidence)";
        color=green;
        style=filled;
        fillcolor="#e8f5e8";
        
        imagenet_results [label="ImageNet Results (ICLR 2022)\nODConv vs CBAM:\n‚Ä¢ MobileNetV2: +3.77% Top-1\n‚Ä¢ ResNet50: +5.71% Top-1\n‚Ä¢ ResNet101: +4.23% Top-1\n\nMS-COCO Detection:\n‚Ä¢ +1.86% to +3.72% mAP improvements", 
                         fillcolor="#c8e6c9"];
        
        widerface_expected [label="WIDERFace Expected (FeatherFace)\nODConv vs CBAM Baseline:\n‚Ä¢ Easy: 94.0% vs 92.7% (+1.3%)\n‚Ä¢ Medium: 92.0% vs 90.7% (+1.3%)\n‚Ä¢ Hard: 80.5% vs 78.3% (+2.2%)\n‚Ä¢ Overall: 88.8% vs 87.2% (+1.6%)\n‚Ä¢ Parameters: 485K vs 488.7K (-0.8%)", 
                           fillcolor="#c8e6c9"];
    }
    
    // Advantages Analysis
    subgraph cluster_advantages {
        label="üéØ Key Advantages Analysis";
        color=purple;
        style=filled;
        fillcolor="#f3e5f5";
        
        cbam_advantages [label="CBAM Advantages\n‚úì Established baseline (ECCV 2018)\n‚úì Simple 2D attention\n‚úì Widely adopted\n‚úì Good performance on standard tasks\n\nLimitations:\n‚ùå Only 2D attention (channel + spatial)\n‚ùå Cannot model long-range dependencies\n‚ùå Higher parameter overhead\n‚ùå Limited multidimensional modeling", 
                        fillcolor="#e1bee7"];
        
        odconv_advantages [label="ODConv Advantages\n‚úÖ 4D multidimensional attention\n‚úÖ Superior long-range modeling\n‚úÖ Proven performance gains (+3-5%)\n‚úÖ Parameter efficient\n‚úÖ Mobile optimized\n‚úÖ ICLR 2022 spotlight paper\n‚úÖ Literature validated (2025 review)\n\nInnovations:\nüöÄ Kernel-wise attention\nüöÄ Omni-dimensional modeling\nüöÄ Dynamic convolution", 
                          fillcolor="#ce93d8"];
    }
    
    // Mathematical Formulation
    subgraph cluster_math {
        label="üìê Mathematical Formulation Comparison";
        color=teal;
        style=filled;
        fillcolor="#e0f2f1";
        
        cbam_math [label="CBAM Mathematics\n\nChannel Attention:\nMc(F) = œÉ(MLP(AvgPool(F)) + MLP(MaxPool(F)))\n\nSpatial Attention:\nMs(F) = œÉ(Conv7√ó7([AvgPool(F); MaxPool(F)]))\n\nFinal Output:\nF' = Ms(F) ‚äó Mc(F) ‚äó F\n\nComplexity: O(C¬≤/r + H√óW)", 
                  fillcolor="#b2dfdb"];
        
        odconv_math [label="ODConv Mathematics\n\nInput: X ‚àà ‚Ñù·¥ÆÀ£·∂ú‚Å±À£·¥¥À£·µÇ, Kernel W ‚àà ‚Ñù·∂ú·µíÀ£·∂ú‚Å±À£·¥¥·µèÀ£·µÇ·µè\n\n4D Attention Generation:\nŒ±À¢ = œÉ(FC‚ÇÅ(GAP(X))) ‚àà ‚Ñù·¥¥·µèÀ£·µÇ·µè\nŒ±‚Å± = œÉ(FC‚ÇÇ(GAP(X))) ‚àà ‚Ñù·∂ú‚Å±\nŒ±·µí = œÉ(FC‚ÇÉ(GAP(X))) ‚àà ‚Ñù·∂ú·µí\nŒ±·µè = œÉ(FC‚ÇÑ(GAP(X))) ‚àà ‚Ñù·¥∑\n\nKernel Modulation:\nWÃÉ = W ‚äô Œ±À¢ ‚äô Œ±‚Å± ‚äô Œ±·µí ‚äô Œ±·µè\n\nDynamic Convolution:\nY = Conv(X, WÃÉ)\n\nComplexity: O(C√óR√ó4) where R << C", 
                   fillcolor="#b2dfdb"];
    }
    
    // Literature Review Summary
    subgraph cluster_literature {
        label="üìö Literature Review Summary 2025";
        color=darkblue;
        style=filled;
        fillcolor="#e8eaf6";
        
        review_summary [label="Systematic Literature Review Findings\n\nüîç Search Methodology:\n‚Ä¢ ICLR, CVPR, ECCV 2020-2025\n‚Ä¢ Attention mechanisms for face detection\n‚Ä¢ False positive reduction focus\n‚Ä¢ Mobile optimization criteria\n\nüìä Key Findings:\n‚Ä¢ ODConv superior to CBAM (ICLR 2022)\n‚Ä¢ SCCA promising for 2025 (Nature Scientific Reports)\n‚Ä¢ SCSA synergistic effects (Neurocomputing 2025)\n‚Ä¢ Consistent ODConv performance gains\n\n‚úÖ Selection Rationale:\n‚Ä¢ Proven performance: +3.77-5.71% ImageNet\n‚Ä¢ Parameter efficiency: Better than CBAM\n‚Ä¢ Long-range modeling: Superior capability\n‚Ä¢ Scientific validation: Top-tier venue\n‚Ä¢ Implementation ready: Available code", 
                       fillcolor="#c5cae9", fontsize=10];
    }
    
    // Connections
    title -> cbam_overview;
    title -> odconv_overview;
    
    cbam_overview -> cbam_channel;
    cbam_overview -> cbam_spatial;
    cbam_channel -> cbam_flow;
    cbam_spatial -> cbam_flow;
    cbam_flow -> cbam_complexity;
    
    odconv_overview -> odconv_spatial;
    odconv_overview -> odconv_input;
    odconv_overview -> odconv_output;
    odconv_overview -> odconv_kernel;
    odconv_spatial -> odconv_flow;
    odconv_input -> odconv_flow;
    odconv_output -> odconv_flow;
    odconv_kernel -> odconv_flow;
    odconv_flow -> odconv_complexity;
    
    cbam_complexity -> imagenet_results [style=dashed, color=red];
    odconv_complexity -> imagenet_results [style=dashed, color=green];
    
    imagenet_results -> widerface_expected;
    
    cbam_overview -> cbam_advantages;
    odconv_overview -> odconv_advantages;
    
    cbam_flow -> cbam_math;
    odconv_flow -> odconv_math;
    
    // Legend
    subgraph cluster_legend {
        label="üîë Decision Matrix";
        color=gray;
        style=filled;
        fillcolor="#fafafa";
        
        decision [label="ODConv Selection Justification\n\nüèÜ Performance: ICLR 2022 proven gains\nüîß Efficiency: Better parameter utilization\nüì± Mobile: Optimized for edge deployment\nüî¨ Science: Top-tier venue validation\nüìä Evidence: Systematic literature review\nüéØ Innovation: 4D vs 2D attention\n\nConclusion: ODConv replaces CBAM baseline\nfor FeatherFace performance optimization", 
                  fillcolor="#f0f0f0", fontsize=10];
    }
}