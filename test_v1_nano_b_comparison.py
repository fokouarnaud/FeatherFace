#!/usr/bin/env python3
"""
FeatherFace V1 vs Nano-B Comprehensive Comparison
Evaluates V1 (teacher) and Nano-B (ultra-lightweight student) architectures

Scientific Evaluation:
1. Parameter efficiency validation
2. Inference speed benchmarks  
3. Memory usage analysis
4. Accuracy maintenance verification
5. Mobile deployment readiness

Scientific Foundation:
- V1: Standard RetinaFace implementation (487K params)
- Nano-B: B-FPGM + Weighted Distillation (120-180K params, 48-65% reduction)
"""

import argparse
import torch
import torch.nn as nn
import time
import psutil
import numpy as np
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple
import json
from collections import OrderedDict

# Add project root to path
PROJECT_ROOT = Path(__file__).parent
sys.path.append(str(PROJECT_ROOT))

# Model imports
from models.retinaface import RetinaFace
from models.featherface_nano_b import FeatherFaceNanoB
from data.config import cfg_mnet, cfg_nano_b
from data.wider_face import WiderFaceDetection, detection_collate
from torch.utils.data import DataLoader
from utils.augmentations import SSDAugmentation


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='FeatherFace V1 vs Nano vs Nano-B Comparison')
    
    # Model paths
    parser.add_argument('--v1_model', 
                       help='Path to V1 model weights')
    # Nano model argument removed - only V1 and Nano-B supported
    parser.add_argument('--nano_b_model',
                       help='Path to Nano-B model weights')
    
    # Test configuration
    parser.add_argument('--test_dataset',
                       help='Test dataset path (optional)')
    parser.add_argument('--batch_size', type=int, default=1,
                       help='Batch size for inference testing')
    parser.add_argument('--num_test_batches', type=int, default=100,
                       help='Number of batches for speed testing')
    parser.add_argument('--warmup_batches', type=int, default=10,
                       help='Number of warmup batches')
    
    # Benchmark settings
    parser.add_argument('--benchmark_inference', action='store_true', default=True,
                       help='Run inference speed benchmarks')
    parser.add_argument('--benchmark_memory', action='store_true', default=True,
                       help='Run memory usage benchmarks')
    parser.add_argument('--validate_accuracy', action='store_true',
                       help='Validate output accuracy (requires test dataset)')
    
    # Device settings
    parser.add_argument('--device', default='auto', choices=['auto', 'cuda', 'cpu'],
                       help='Device to use for testing')
    parser.add_argument('--precision', default='float32', choices=['float32', 'float16'],
                       help='Precision for inference')
    
    # Output settings
    parser.add_argument('--save_results', action='store_true',
                       help='Save benchmark results to JSON')
    parser.add_argument('--output_dir', default='results/',
                       help='Directory to save results')
    
    return parser.parse_args()


class ModelComparator:
    """
    Comprehensive model comparison tool for FeatherFace architectures
    """
    
    def __init__(self, args):
        """Initialize comparator"""
        self.args = args
        self.device = self._setup_device()
        self.precision = args.precision
        
        # Results storage
        self.results = {
            'v1': {},
            'nano_b': {},
            'comparisons': {}
        }
        
        # Create output directory
        if args.save_results:
            Path(args.output_dir).mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ”¬ FeatherFace Model Comparator")
        print(f"ğŸ“Š Device: {self.device}")
        print(f"ğŸ¯ Precision: {self.precision}")
        print("=" * 60)
    
    def _setup_device(self):
        """Setup computation device"""
        if self.args.device == 'auto':
            return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        return torch.device(self.args.device)
    
    def load_models(self) -> Dict[str, nn.Module]:
        """Load all available models"""
        models = {}
        
        # Load V1 model
        print("\nğŸ“‚ Loading Models...")
        
        try:
            v1_model = RetinaFace(cfg=cfg_mnet, phase='test')
            if self.args.v1_model and os.path.exists(self.args.v1_model):
                checkpoint = torch.load(self.args.v1_model, map_location='cpu')
                if 'model_state_dict' in checkpoint:
                    v1_model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    v1_model.load_state_dict(checkpoint)
                print(f"âœ… V1 model loaded from {self.args.v1_model}")
            else:
                print("âš ï¸  V1 model: Using random weights (no checkpoint provided)")
            
            v1_model = v1_model.to(self.device).eval()
            models['v1'] = v1_model
            
        except Exception as e:
            print(f"âŒ Failed to load V1 model: {e}")
        
        # Nano model loading removed - only V1 and Nano-B supported
        
        # Load Nano-B model
        try:
            nano_b_model = FeatherFaceNanoB(cfg=cfg_nano_b, phase='test')
            if self.args.nano_b_model and os.path.exists(self.args.nano_b_model):
                checkpoint = torch.load(self.args.nano_b_model, map_location='cpu')
                if 'model_state_dict' in checkpoint:
                    nano_b_model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    nano_b_model.load_state_dict(checkpoint)
                print(f"âœ… Nano-B model loaded from {self.args.nano_b_model}")
            else:
                print("âš ï¸  Nano-B model: Using random weights (no checkpoint provided)")
            
            nano_b_model = nano_b_model.to(self.device).eval()
            models['nano_b'] = nano_b_model
            
        except Exception as e:
            print(f"âŒ Failed to load Nano-B model: {e}")
        
        print(f"\nğŸ“Š Loaded {len(models)} models successfully")
        return models
    
    def analyze_model_parameters(self, models: Dict[str, nn.Module]):
        """Analyze parameter counts and efficiency"""
        print("\nğŸ”¬ Parameter Analysis")
        print("=" * 60)
        
        for name, model in models.items():
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            # Model size estimation (float32)
            model_size_mb = total_params * 4 / (1024 * 1024)
            
            self.results[name]['parameters'] = {
                'total': total_params,
                'trainable': trainable_params,
                'size_mb': model_size_mb
            }
            
            print(f"ğŸ“Š {name.upper()} Model:")
            print(f"  Parameters: {total_params:,}")
            print(f"  Trainable: {trainable_params:,}")
            print(f"  Size: {model_size_mb:.2f} MB")
            
            # Calculate efficiency metrics
            if name != 'v1':  # V1 is baseline
                v1_params = self.results['v1']['parameters']['total']
                reduction = ((v1_params - total_params) / v1_params) * 100
                compression_ratio = v1_params / total_params
                
                print(f"  Reduction: {reduction:.1f}%")
                print(f"  Compression: {compression_ratio:.1f}x")
                
                self.results[name]['efficiency'] = {
                    'reduction_percent': reduction,
                    'compression_ratio': compression_ratio
                }
            
            print()
    
    def benchmark_inference_speed(self, models: Dict[str, nn.Module]):
        """Benchmark inference speed for all models"""
        if not self.args.benchmark_inference:
            return
        
        print("\nâš¡ Inference Speed Benchmark")
        print("=" * 60)
        
        # Create dummy input
        dummy_input = torch.randn(self.args.batch_size, 3, 640, 640).to(self.device)
        
        if self.precision == 'float16':
            dummy_input = dummy_input.half()
        
        for name, model in models.items():
            print(f"ğŸš€ Testing {name.upper()} model...")
            
            if self.precision == 'float16':
                model = model.half()
            
            # Warmup
            print(f"  Warming up ({self.args.warmup_batches} batches)...")
            with torch.no_grad():
                for _ in range(self.args.warmup_batches):
                    _ = model(dummy_input)
                    if self.device.type == 'cuda':
                        torch.cuda.synchronize()
            
            # Benchmark
            print(f"  Benchmarking ({self.args.num_test_batches} batches)...")
            times = []
            
            with torch.no_grad():
                for _ in range(self.args.num_test_batches):
                    start_time = time.time()
                    outputs = model(dummy_input)
                    
                    if self.device.type == 'cuda':
                        torch.cuda.synchronize()
                    
                    end_time = time.time()
                    times.append(end_time - start_time)
            
            # Calculate statistics
            times = np.array(times) * 1000  # Convert to milliseconds
            mean_time = np.mean(times)
            std_time = np.std(times)
            min_time = np.min(times)
            max_time = np.max(times)
            fps = 1000 / mean_time
            
            self.results[name]['inference'] = {
                'mean_ms': mean_time,
                'std_ms': std_time,
                'min_ms': min_time,
                'max_ms': max_time,
                'fps': fps,
                'batch_size': self.args.batch_size
            }
            
            print(f"  Mean: {mean_time:.2f}ms Â± {std_time:.2f}ms")
            print(f"  Range: {min_time:.2f}ms - {max_time:.2f}ms")
            print(f"  FPS: {fps:.1f}")
            print()
    
    def benchmark_memory_usage(self, models: Dict[str, nn.Module]):
        """Benchmark memory usage for all models"""
        if not self.args.benchmark_memory:
            return
        
        print("\nğŸ’¾ Memory Usage Benchmark")
        print("=" * 60)
        
        dummy_input = torch.randn(self.args.batch_size, 3, 640, 640).to(self.device)
        
        for name, model in models.items():
            print(f"ğŸ“Š Testing {name.upper()} memory usage...")
            
            # Clear cache
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
            
            # Measure baseline memory
            if self.device.type == 'cuda':
                baseline_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            else:
                baseline_memory = psutil.virtual_memory().used / 1024 / 1024  # MB
            
            # Forward pass
            with torch.no_grad():
                outputs = model(dummy_input)
            
            # Measure peak memory
            if self.device.type == 'cuda':
                peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
                torch.cuda.reset_peak_memory_stats()
            else:
                peak_memory = psutil.virtual_memory().used / 1024 / 1024  # MB
            
            memory_used = peak_memory - baseline_memory
            
            self.results[name]['memory'] = {
                'baseline_mb': baseline_memory,
                'peak_mb': peak_memory,
                'used_mb': memory_used,
                'batch_size': self.args.batch_size
            }
            
            print(f"  Baseline: {baseline_memory:.1f} MB")
            print(f"  Peak: {peak_memory:.1f} MB")
            print(f"  Used: {memory_used:.1f} MB")
            print()
    
    def compare_outputs(self, models: Dict[str, nn.Module]):
        """Compare model outputs for consistency"""
        print("\nğŸ” Output Consistency Check")
        print("=" * 60)
        
        dummy_input = torch.randn(1, 3, 640, 640).to(self.device)
        outputs = {}
        
        # Get outputs from all models
        with torch.no_grad():
            for name, model in models.items():
                try:
                    output = model(dummy_input)
                    outputs[name] = output
                    print(f"âœ… {name.upper()}: {len(output) if isinstance(output, (list, tuple)) else 1} outputs")
                except Exception as e:
                    print(f"âŒ {name.upper()}: Failed - {e}")
        
        # Compare output shapes
        if len(outputs) > 1:
            print("\nğŸ“ Output Shape Comparison:")
            reference_name = list(outputs.keys())[0]
            reference_output = outputs[reference_name]
            
            for name, output in outputs.items():
                if name != reference_name:
                    if isinstance(output, (list, tuple)) and isinstance(reference_output, (list, tuple)):
                        if len(output) == len(reference_output):
                            shape_match = all(o.shape == r.shape for o, r in zip(output, reference_output))
                            print(f"  {name.upper()} vs {reference_name.upper()}: {'âœ… Shapes match' if shape_match else 'âŒ Shapes differ'}")
                        else:
                            print(f"  {name.upper()} vs {reference_name.upper()}: âŒ Different number of outputs")
    
    def generate_comparison_summary(self):
        """Generate comprehensive comparison summary"""
        print("\nğŸ“Š COMPREHENSIVE COMPARISON SUMMARY")
        print("=" * 80)
        
        # Parameter efficiency comparison
        if 'v1' in self.results and 'nano_b' in self.results:
            v1_params = self.results['v1']['parameters']['total']
            nano_b_params = self.results['nano_b']['parameters']['total']
            nano_b_reduction = ((v1_params - nano_b_params) / v1_params) * 100
            
            print(f"ğŸ¯ Parameter Efficiency:")
            print(f"  V1 â†’ Nano-B: {v1_params:,} â†’ {nano_b_params:,} ({nano_b_reduction:.1f}% reduction)")
        
        # Speed comparison
        if all(model in self.results and 'inference' in self.results[model] 
               for model in ['v1', 'nano_b']):
            print(f"\nâš¡ Inference Speed (FPS):")
            for model in ['v1', 'nano_b']:
                fps = self.results[model]['inference']['fps']
                print(f"  {model.upper()}: {fps:.1f} FPS")
            
            # Calculate speedup
            v1_fps = self.results['v1']['inference']['fps']
            nano_b_speedup = self.results['nano_b']['inference']['fps'] / v1_fps
            print(f"  Nano-B speedup: {nano_b_speedup:.2f}x")
        
        # Memory efficiency
        if all(model in self.results and 'memory' in self.results[model] 
               for model in ['v1', 'nano_b']):
            print(f"\nğŸ’¾ Memory Usage (MB):")
            for model in ['v1', 'nano_b']:
                memory = self.results[model]['memory']['used_mb']
                print(f"  {model.upper()}: {memory:.1f} MB")
        
        # Scientific validation
        print(f"\nğŸ”¬ Scientific Validation:")
        print(f"  âœ… V1 Baseline: Standard RetinaFace implementation")
        print(f"  âœ… Nano-B: B-FPGM + Weighted Distillation (7 techniques)")
        
        # Target achievement
        if 'nano_b' in self.results:
            nano_b_params = self.results['nano_b']['parameters']['total']
            target_min = 120000
            target_max = 180000
            
            if target_min <= nano_b_params <= target_max:
                print(f"  âœ… Nano-B Target: {nano_b_params:,} params (within {target_min//1000}K-{target_max//1000}K range)")
            else:
                print(f"  âš ï¸  Nano-B Target: {nano_b_params:,} params (outside {target_min//1000}K-{target_max//1000}K range)")
    
    def save_results_to_file(self):
        """Save benchmark results to JSON file"""
        if not self.args.save_results:
            return
        
        # Add metadata
        self.results['metadata'] = {
            'device': str(self.device),
            'precision': self.precision,
            'batch_size': self.args.batch_size,
            'num_test_batches': self.args.num_test_batches,
            'warmup_batches': self.args.warmup_batches,
            'timestamp': time.strftime('%Y-%m-%d_%H-%M-%S')
        }
        
        # Save to file
        output_file = os.path.join(self.args.output_dir, 'featherface_comparison_results.json')
        
        with open(output_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        print(f"\nğŸ’¾ Results saved to: {output_file}")
    
    def run_comparison(self):
        """Run complete model comparison"""
        print("ğŸš€ Starting FeatherFace Model Comparison...")
        
        # Load models
        models = self.load_models()
        
        if not models:
            print("âŒ No models loaded. Exiting.")
            return
        
        # Run analyses
        self.analyze_model_parameters(models)
        self.benchmark_inference_speed(models)
        self.benchmark_memory_usage(models)
        self.compare_outputs(models)
        
        # Generate summary
        self.generate_comparison_summary()
        
        # Save results
        self.save_results_to_file()
        
        print("\nğŸ‰ Comparison completed successfully!")


def main():
    """Main comparison function"""
    args = parse_args()
    
    comparator = ModelComparator(args)
    comparator.run_comparison()


if __name__ == '__main__':
    main()